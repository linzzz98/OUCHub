Efﬁcient Initial Pose-graph Generation for Global SfM
=======================================================

Abstract
---------
文章提出了加速全局SfM算法的初始位姿图生成的方法。

位姿图创建中最耗时的步骤是由 FLANN 形成暂时的特征点对应关系（不一定正确的对应关系）和由 RANSAC 进行几何验证，为了避免这个步骤，文章提出了两种新方法——基于图像对（通常连续匹配）。
候选的相对姿态可以从部分构建的位姿图的路径中恢复。

考虑到图像的全局相似性和位姿图边缘的质量， 作者提出了 A* 遍历的启发式方法。给定来自路径的相对姿态，通过利用已知的对极几何，基于描述符的特征匹配变得“轻量级”。

为了在应用 RANSAC 时加快基于 PROSAC 的采样，文章提出了第三种方法，根据先前估计的内点概率对对应项进行排序。

Introduction
-------------

.. figure:: 1.jpg
   :figclass: align-center

   global sfm pipeline


过程如下：

1. 在所有图像中提取特征。这个步骤很容易并行化，并且具有O(n)个时间复杂度，其中n是重建中要包含的图像的数量。

2. 这些特征通常用于将图像对从最可能匹配到最难匹配的图像对排序，例如，通过视觉词袋。

3. 通过匹配检测到的特征点的通常高维（例如，128维度的SIFT）描述符，在所有图像对之间产生暂时的对应。

4. 通过应用RANSAC来过滤对应关系并估计所有图像对之间的相对姿态。

.. note::

   通常，特征匹配和几何估计步骤是迄今为止最慢的部分，两者都对图像的数量具有二次复杂度。此外，特征匹配具有二次最坏情况的时间复杂度，因为它取决于各自图像中特征数的乘积。

5. 全局集束调整（BA）得到了成对位姿的精确重构。与初始的位姿图生成相比，这个步骤有可以忽略不计的时间需求。

本文有三个主要贡献————三种新算法，可以消除基于RANSAC的几何估计的需要，并使基于描述的特征匹配“轻量级”

1. 提出了一种利用部分建立的位姿图来避免基于计算要求的基于RANSAC的鲁棒估计的方法。为此为 **A∗算法** 提出了一个启发式算法，即使没有视图之间的度量距离，它也可以指导路径查找。

2. 提出了一种通过使用由A∗确定的位姿来使耗时的基于描述符的特征匹配“轻量级”的技术。这种引导匹配方法使用基本矩阵有效地选择关键点，从而通过hashing与位姿一致的对应。

3. 提出了一种算法，根据它们的历史对点对点的对应关系进行自适应重新排序——无论这些点中的一个还是两个在之前的估计中都是内点。该方法利用了这样一个事实，即这些内部特征点可能代表与场景刚性重建一致的 3D 点。 这种自适应排序通过指导 PROSAC 尽早找到一个好的样本来加速稳健的估计。

Related tech
~~~~~~~~~~~~

**鲁棒估计** ：PROSAC 利用点的先验预测内点概率等级，并从最有希望的点开始采样。NAPSAC 建立在现实世界数据通常在空间上是一致的这一事实之上，并从局部邻域中选择样本，在这些邻域中，内点比率可能很高。P-NAPSAC 结合了 PROSAC 和 NAPSAC 的优点，首先在本地采样，然后逐渐融合到全局采样中。
序列概率比率测试(SPRT) 用于在优于先前最佳模型的概率低于阈值时尽早拒绝模型。所有提到的 RANSAC 改进都考虑了单个、孤立的两视图鲁棒估计的情况。

**特征匹配** ：通过多种方式加速。

**全局图像的相似性** ：与匹配（例如视频序列）相比，匹配无序图像集合通常是一项更难、更耗时的任务。这有两个原因。首先，许多图像对可能没有场景的任何常见可见部分，并且浪费了在匹配尝试上花费的时间。此外，不匹配是 RANSAC 的最坏情况，它将运行最大迭代次数，通常比可能匹配的情况多几个数量级。其次，估计对极几何所花费的时间在很大程度上取决于暂定对应关系的内点比率。
可以重用提取的局部特征，通过视觉词袋找到最有希望的候选匹配，然后使用几何约束快速重新排序初步列表，此类系统运行良好，但内存占用很大。这个问题现在已被基于 CNN 的全局描述符所克服，它们计算速度更快并提供更准确的结果。

作为初步步骤，使用以下方法生成完全连接的图像相似度图。

1. 使用 ResNet-50 CNN 提取 GeM描述符，在 GLD-v1 数据集上进行预训练。然后我们计算所有描述符之间的内积相似度，得到一个 n × n 相似度矩阵。相似度矩阵的计算是该pipeline的唯一二次步骤。但是，标量乘积运算速度极快。实际上，相似度矩阵的创建和处理花费的时间可以忽略不计。

Relative Pose from Directed  Walks
-----------------------------------

本文提出了一种通过尽可能避免运行 RANSAC 来加速姿势图生成的方法。

核心思想利用了这样一个事实，即在从图像集合中估计第 (t+1) 个图像对之间的相对位姿时，会得到一个由 t 个边（即 t 个视图对）组成的位姿图。该位姿图通常可用于估计姿势，而无需运行类似 RANSAC 的稳健估计。

在接下来的描述中，假设视图对是按照它们的相似度得分排序的。因此，从最相似的视图对开始姿态估计。让我们假设我们已经成功匹配了 t 个图像对，因此得到了位姿图： :math:`\mathcal{G}_t = \{V,E\}`

.. figure:: 2.jpg
   :figclass: align-center

在估计第 (t + 1) 个视图对之间的相对位姿时有两个选择。

传统的方法是对两幅图像之间的对应点进行鲁棒估计。然后将估计的位姿  :math:`P ∈ SE(3)`  添加到位姿图中作为新边的位姿。 因此， :math:`\mathcal{E}_{t+1} = \mathcal{E}_t ∪ \{e = (v_s, v_d)\}` ，并且 :math:`\phi(e) = P`
这一步的问题在于，当内点很少，因此内点比率较低时，估计通常很耗时。

因此，本文建议使用先前生成的姿势图  :math:`\mathcal{G}_t` ，而不是在一对视图  :math:`(v_s, v_d)` 之间盲目地估计位姿。

.. figure:: 3.jpg
   :figclass: align-center

   文章使用的所有符号

.. figure:: 4.jpg
   :figclass: align-center

将  :math:`\mathcal{W}` 隐含的位姿递归地定义为：

.. figure:: 5.jpg
   :figclass: align-center

因此，给定有限的步行  :math:`\mathcal{W}` ，视图  :math:`v_s` 和 :math:`v_d` 之间的相对位姿计算为 :math:`\phi(\mathcal{W})` 。

上面公式的问题在于单个错误估计的位姿，使整个 :math:`\phi(\mathcal{W})` 错误。因此，需要在给定距离内找到多次步行，即限制最大深度以避免无限长的步行，返回的步行按照顺序立刻评估：

.. figure:: 6.jpg
   :figclass: align-center
