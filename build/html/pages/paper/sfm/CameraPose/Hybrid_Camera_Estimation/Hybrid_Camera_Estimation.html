

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>13. Hybrid Camera Pose Estimation &mdash; OUCHub  文档</title>
  

  
  <link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/twemoji.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/custom.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../../" src="../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../_static/jquery.js"></script>
        <script src="../../../../../_static/underscore.js"></script>
        <script src="../../../../../_static/doctools.js"></script>
        <script src="../../../../../_static/language_data.js"></script>
        <script src="https://twemoji.maxcdn.com/v/latest/twemoji.min.js"></script>
        <script src="../../../../../_static/twemoji.js"></script>
        <script src="../../../../../_static/translations.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="索引" href="../../../../../genindex.html" />
    <link rel="search" title="搜索" href="../../../../../search.html" />
    <link rel="next" title="14. Optimizing the Viewing Graph for Structure-from-Motion" href="../../LocalMethod/Global/Optimizing_the_Viewing_Graph/Optimizing_the_Viewing_Graph.html" />
    <link rel="prev" title="12. Global Structure-from-Motion by Similarity Averaging" href="../../LocalMethod/Global/GlobalSfMSA/GlobalSfMSA.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: black" >
          

          
            <a href="../../../../../index.html" class="icon icon-home"> OUCHub
          

          
            
            <img src="../../../../../_static/logo_1.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">基础知识</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../knowledge/k_MH.html">💊 Math</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../knowledge/k_CV.html">🍤 Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../knowledge/k_ML.html">🍎 Machine Learning</a></li>
</ul>
<p class="caption"><span class="caption-text">论文学习</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../../p_sfm.html">🍊 Structure from Motion</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../LocalMethod/Incremental/Structure-from-Motion_Revisited/Structure-from-Motion_Revisited.html">1. Structure-from-Motion Revisited</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../LocalMethod/Incremental/Hierarchical_Structure-And-Motion/Hierarchical_Structure-And-Motion.html">2. Hierarchical structure-and-motion recovery from uncalibrated images</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../LocalMethod/Hybrid/HSfM/HSfM.html">3. HSfM: Hybrid Structure-from-Motion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../A_Global_Linear_Method_for_Camera_Pose_Registration/A_Global_Linear_Method_for_Camera_Pose_Registration.html">4. A Global Linear Method for Camera Pose Registration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Robust_Rotation_and_Translation_Estimation/Robust_Rotation_and_Translation_Estimation.html">5. Robust Rotation and Translation Estimation in Multiview Reconstruction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../LocalMethod/Hybrid/View_Graph_Construction/View_Graph_Construction.html">6. View-graph construction framework for robust and efficient structure-from-motion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../FivePoint_Relative_Pose_Problem/FivePoint_Relative_Pose_Problem.html">7. An Effcient Solution to the Five-Point Relative Pose Problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../DeepLearning/SfMLearner/SfMLearner.html">8. Unsupervised Learning of Depth and Ego-Motion from Video</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../DeepLearning/DeepSfM/DeepSfM.html">9. DeepSFM: Structure From Motion Via Deep Bundle Adjustment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../LocalMethod/Global/Very_Large-Scale_Global_SfM/Very_Large-Scale_Global_SfM.html">10. Very Large-Scale Global SfM by Distributed Motion Averaging</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../LocalMethod/Incremental/MarkerSfM/MarkerSfM.html">11. Improved Structure from Motion Using Fiducial Marker Matching</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../LocalMethod/Global/GlobalSfMSA/GlobalSfMSA.html">12. Global Structure-from-Motion by Similarity Averaging</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">13. Hybrid Camera Pose Estimation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#introduction">13.1. Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#background">13.2. Background</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#structure-based-pose-estimation">13.2.1. Structure-based pose estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#structure-less-pose-estimation">13.2.2. Structure-less pose estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#hybrid-pose-estimation">13.2.3. Hybrid pose estimation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#hybrid-ransac-for-pose-estimation">13.3. Hybrid RANSAC for Pose Estimation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../LocalMethod/Global/Optimizing_the_Viewing_Graph/Optimizing_the_Viewing_Graph.html">14. Optimizing the Viewing Graph for Structure-from-Motion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../p_pointcloud.html">🍋 Point Cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../p_others.html">🍒 Others</a></li>
</ul>
<p class="caption"><span class="caption-text">源码解析</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../code/c_colmap.html">🍑 Colmap</a></li>
</ul>
<p class="caption"><span class="caption-text">杂七杂八</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../others/o_others.html">🍺 Others</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">OUCHub</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../p_sfm.html">🍊 Structure from Motion</a> &raquo;</li>
        
      <li><span class="section-number">13. </span>Hybrid Camera Pose Estimation</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../../../_sources/pages/paper/sfm/CameraPose/Hybrid_Camera_Estimation/Hybrid_Camera_Estimation.rst.txt" rel="nofollow"> 查看页面源码</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="hybrid-camera-pose-estimation">
<h1><span class="section-number">13. </span>Hybrid Camera Pose Estimation<a class="headerlink" href="#hybrid-camera-pose-estimation" title="永久链接至标题">¶</a></h1>
<p>本文旨在解决经校准的针孔相机和通用相机的位姿估计问题。 通过利用2D-3D对应以及2D-2D对应来构建SfM模型。</p>
<p>传统方法要么专注于使用2D-3D匹配（称为基于结构的位姿估计（structure-based）），要么仅专注于2D-2D匹配（无结构的位姿估计（structure-less））。</p>
<p>绝对位姿估计方法的性能受到3D点的三角剖分的质量以及3D模型完整性的限制。另一方面，相对位姿估计方法虽然更准确，但在计算上也往往要昂贵得多，并且经常返回许多可能的解。</p>
<p>本文工作旨在弥合这两种范式之间的鸿沟。提出了一种基于RANSAC的新方法，该方法可以自动选择最佳类型的求解器，以数据驱动的方式在每次迭代中使用。
RANSAC选择的求解器的范围从纯基于结构的求解器或无结构的求解器到混合求解器的任何可能组合（即使用两种匹配类型）。</p>
<div class="section" id="introduction">
<h2><span class="section-number">13.1. </span>Introduction<a class="headerlink" href="#introduction" title="永久链接至标题">¶</a></h2>
<p>相机位姿估计的传统方法是根据查询图像中像素与场景模型中3D点之间的一组2D-3D匹配来估计姿态。通常通过在RANSAC循环中应用基于结构的最小位姿求解器来计算位姿。
根据n个2D-3D匹配进行绝对位姿估计的工作量很大，这个问题通常称为n点透视问题（PnP）。
当通过SfM获得3D模型时，并非是所有3D点都将被精确的三角剖分，从而导致潜在的不准确的位姿估计。</p>
<p>从2D-3D匹配中进行位姿估计的另一种方法是无结构位姿resectioning：查询图像（query image）的位姿是根据查询图像与重建中的两个或多个图像之间的一组2D-2D对应关系来估计的。
虽然这种方法避免了三角剖分点不正确和模型不完整的问题，但无结构的位姿求解器的计算效率却大大降低了（慢了几个数量级）。</p>
<p>本文提出了九种新颖的混合摄像机位姿求解器，它们的区别在于每次使用的2D-3D和2D-2D匹配项的数量以及它们是针对中央摄像机还是通用摄像机（或两者）进行处理。、</p>
</div>
<div class="section" id="background">
<h2><span class="section-number">13.2. </span>Background<a class="headerlink" href="#background" title="永久链接至标题">¶</a></h2>
<div class="section" id="structure-based-pose-estimation">
<h3><span class="section-number">13.2.1. </span>Structure-based pose estimation<a class="headerlink" href="#structure-based-pose-estimation" title="永久链接至标题">¶</a></h3>
<p>在基于结构的位姿估计中遵循的经过程包括：</p>
<p>首先获得一组假定的2D-3D匹配项。 这些推定的匹配通常是通过将查询图像（或图像序列）中的关键点与与模型中每个3D点相关联的平均描述子进行匹配而获得的。
给定这些匹配项，通过在“假设和验证”方案（例如RANSAC）中采用最小求解器，可以可靠地推断出相机的位姿。</p>
</div>
<div class="section" id="structure-less-pose-estimation">
<h3><span class="section-number">13.2.2. </span>Structure-less pose estimation<a class="headerlink" href="#structure-less-pose-estimation" title="永久链接至标题">¶</a></h3>
<p>通过基于结构的方法获得的位姿精度受到相机观察到的3D点的质量限制。
另外，查询相机可以拥有的inlier matches的数量最多为摄像机看到的3D点的数量。
不是通过3D结构来明确表示场景，而是使用查询图像（或多个图像）与多个图像之间的2D-2D对应关系来进行位姿估计。 在这些情况下，查询图像通常与使用图像检索方法找到的SfM模型中存在的最相似图像进行匹配。</p>
<p>同样的，还可以通过遵循针对基于结构的位姿估计所描述的相同过程来获得2D-2D匹配。
一旦获得一组2D-3D匹配，就可以将每个匹配的3D对应替换为用于重建此3D点的SfM相机光线之一。（啥意思？） 接下来，在RANSAC循环内使用最小解算器，以便可靠地估计摄像机的位姿。</p>
</div>
<div class="section" id="hybrid-pose-estimation">
<h3><span class="section-number">13.2.3. </span>Hybrid pose estimation<a class="headerlink" href="#hybrid-pose-estimation" title="永久链接至标题">¶</a></h3>
<p>介于以上两者之间的，有一些求解器同时使用不同类型的匹配项，这被称为混合求解器。</p>
</div>
</div>
<div class="section" id="hybrid-ransac-for-pose-estimation">
<h2><span class="section-number">13.3. </span>Hybrid RANSAC for Pose Estimation<a class="headerlink" href="#hybrid-ransac-for-pose-estimation" title="永久链接至标题">¶</a></h2>
<p>令 <span class="math notranslate nohighlight">\(\mathcal{M}_p\)</span> 为一组推定的2D-3D匹配，而:math:<cite>mathcal{M}_r</cite> 为一组推定的2D-2D匹配。</p>
<p>传统方法是仅使用这些集合中的一组使用RANSAC与最小求解器（如P3P）或2D-2D无结构位姿求解器组合来鲁棒地估计查询相机的位姿。</p>
<p>但是，我们想利用所有可用信息，例如通过使用混合最小求解器，该求解器同时使用2D-3D和2D-2D匹配项。为了使用这样的求解器，RANSAC需要同时从 <span class="math notranslate nohighlight">\(\mathcal{M}_p\)</span> 和 <span class="math notranslate nohighlight">\(\mathcal{M}_r\)</span> 那里取样。</p>
<p>因此，其终止准则应适用于两个独立的inlier ratios，将其称为 <span class="math notranslate nohighlight">\(\epsilon_r\)</span> 和 <span class="math notranslate nohighlight">\(\epsilon_p\)</span> 。</p>
<p>设计可以处理两组不同的匹配项和inlier ratios以及许多不同的最小解算器的RANSAC变体时，会出现三个问题：</p>
<ol class="arabic simple">
<li><p>如何选择到目前为止最好的模型？</p></li>
<li><p>给定估计的线性比率 <span class="math notranslate nohighlight">\(\epsilon_r\)</span> 和 <span class="math notranslate nohighlight">\(\epsilon_p\)</span> ，以及过去选择的求解器，哪一个是用于下一次迭代的最佳求解器？</p></li>
<li><p>什么时候应该终止？</p></li>
</ol>
<p>对于第一个问题，本文采用经典的RANSAC方法，并在考虑了两组匹配的情况下，选择最佳模型作为整体inlier count最高的模型。
注意，可以为每组匹配选择不同的inlier ratios。</p>
<p>在每次迭代中，根据求解器首次在该迭代中成功的概率（从所有内部最小样本估计模型）来选择求解器。</p>
<p>因此，希望选择一个成功机会很高的求解器，而该求解器还没有被广泛使用以找到有效的解决方案。
这意味着它第一次找到好的解决方案的机会应该很高。</p>
<p>将这种概率称为成功概率，即 <span class="math notranslate nohighlight">\(P_s\)</span> 。</p>
<p>假设 <span class="math notranslate nohighlight">\(s\)</span> 是一个求解器，它要求 <span class="math notranslate nohighlight">\(\mathcal{M}_r\)</span> 最少进行 <span class="math notranslate nohighlight">\(n\)</span> 次匹配，<span class="math notranslate nohighlight">\(\mathcal{M}_p\)</span> 至少进行 <span class="math notranslate nohighlight">\(m\)</span> 个匹配项。</p>
<p>令 <span class="math notranslate nohighlight">\(\epsilon_r\)</span> 和 <span class="math notranslate nohighlight">\(\epsilon_p\)</span> 分别为集合 <span class="math notranslate nohighlight">\(\mathcal{M}_p\)</span> 和 <span class="math notranslate nohighlight">\(\mathcal{M}_r\)</span> 的真实inlier ratios</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(\epsilon_r^n \epsilon_p^m\)</span> 给出在任何迭代中采样求解器 <span class="math notranslate nohighlight">\(s\)</span> 的所有内点最小值集的概率。</p>
</div></blockquote>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="../../LocalMethod/Global/Optimizing_the_Viewing_Graph/Optimizing_the_Viewing_Graph.html" class="btn btn-neutral float-right" title="14. Optimizing the Viewing Graph for Structure-from-Motion" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="../../LocalMethod/Global/GlobalSfMSA/GlobalSfMSA.html" class="btn btn-neutral float-left" title="12. Global Structure-from-Motion by Similarity Averaging" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; 版权所有 2021, linzzz.

    </p>
  </div> 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>