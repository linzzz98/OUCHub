<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>21. A Global Linear Method for Camera Pose Registration &mdash; OUCHub  文档</title>
      <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/twemoji.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../../../" id="documentation_options" src="../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../_static/jquery.js"></script>
        <script src="../../../../../_static/underscore.js"></script>
        <script src="../../../../../_static/doctools.js"></script>
        <script src="https://twemoji.maxcdn.com/v/latest/twemoji.min.js"></script>
        <script src="../../../../../_static/twemoji.js"></script>
        <script src="../../../../../_static/translations.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../../../../_static/js/theme.js"></script>
    <link rel="index" title="索引" href="../../../../../genindex.html" />
    <link rel="search" title="搜索" href="../../../../../search.html" />
    <link rel="next" title="22. Robust Rotation and Translation Estimation in Multiview Reconstruction" href="../Robust_Rotation_and_Translation_Estimation/Robust_Rotation_and_Translation_Estimation.html" />
    <link rel="prev" title="20. View-graph construction framework for robust and efficient structure-from-motion" href="../../LocalMethod/Hybrid/View_Graph_Construction/View_Graph_Construction.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: black" >
            <a href="../../../../../index.html" class="icon icon-home"> OUCHub
            <img src="../../../../../_static/logo_1.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">基础知识</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../knowledge/k_MH.html">💊 Math</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../knowledge/k_CV.html">🍤 Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../knowledge/k_ML.html">🍎 Machine Learning</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">论文学习</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../../p_sfm.html">🍊 Structure from Motion</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../LocalMethod/Incremental/Structure-from-Motion_Revisited/Structure-from-Motion_Revisited.html">1. Structure-from-Motion Revisited</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../LocalMethod/Incremental/Hierarchical_Structure-And-Motion/Hierarchical_Structure-And-Motion.html">2. Hierarchical structure-and-motion recovery from uncalibrated images</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../LocalMethod/Incremental/View-graph_SfM/View-graph_SfM.html">3. View-graph Selection Framework for SfM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../LocalMethod/Incremental/View-graph_Framework/View-graph_Framework.html">4. View-graph construction framework for robust and efficient structure-from-motion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../LocalMethod/Incremental/MarkerSfM/MarkerSfM.html">5. Improved Structure from Motion Using Fiducial Marker Matching</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../LocalMethod/Incremental/Privacy_SfM/Privacy_SfM.html">6. Privacy Preserving Structure-from-Motion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../LocalMethod/Incremental/SfM_with_Line_Segments/SfM_with_Line_Segments.html">7. Structure from Motion with Line Segments Under Relaxed Endpoint Constraints</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../LocalMethod/Incremental/Line_based_SfM/Line_based_SfM.html">8. Line-based Robust SfM with Little Image Overlap</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../LocalMethod/Incremental/Voting_based_SfM/Voting_based_SfM.html">9. Voting-based Incremental Structure-from-Motion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../LocalMethod/Incremental/Graph_based_SfM/Graph_based_SfM.html">10. Graph-Based Consistent Matching for Structure-from-Motion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../LocalMethod/Global/CSfM/CSfM.html">11. CSFM: COMMUNITY-BASED STRUCTURE FROM MOTION</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../LocalMethod/Global/Very_Large-Scale_Global_SfM/Very_Large-Scale_Global_SfM.html">12. Very Large-Scale Global SfM by Distributed Motion Averaging</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../LocalMethod/Global/GlobalSfM_SA/GlobalSfM_SA.html">13. Global Structure-from-Motion by Similarity Averaging</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../LocalMethod/Global/GlobalSfM_Application/GlobalSfM_Application.html">14. Global Structure-from-Motion and Its Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../LocalMethod/Global/Efficient_Initial_Pose-graph/Efficient_Initial_Pose-graph.html">15. Efficient Initial Pose-graph Generation for Global SfM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../LocalMethod/Global/Optimizing_the_Viewing_Graph/Optimizing_the_Viewing_Graph.html">16. Optimizing the Viewing Graph for Structure-from-Motion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../LocalMethod/Global/Reducing_Drift_Using_Extended_Feature/Reducing_Drift_Using_Extended_Feature.html">17. Reducing Drift in Structure From Motion Using Extended Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../LocalMethod/Global/Rotation_BA/Rotation_BA.html">18. Rotation-Only Bundle Adjustment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../LocalMethod/Hybrid/HSfM/HSfM.html">19. HSfM: Hybrid Structure-from-Motion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../LocalMethod/Hybrid/View_Graph_Construction/View_Graph_Construction.html">20. View-graph construction framework for robust and efficient structure-from-motion</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">21. A Global Linear Method for Camera Pose Registration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#review">21.1. Review</a></li>
<li class="toctree-l3"><a class="reference internal" href="#translation-registration">21.2. Translation Registration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#triplet-translation-registration">21.2.1. Triplet Translation Registration</a></li>
<li class="toctree-l4"><a class="reference internal" href="#collinear-camera-motion">21.2.2. Collinear Camera Motion</a></li>
<li class="toctree-l4"><a class="reference internal" href="#multi-view-translation-registration">21.2.3. Multi-view Translation Registration</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#generalization-to-eg-outliers">21.3. Generalization to EG Outliers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../Robust_Rotation_and_Translation_Estimation/Robust_Rotation_and_Translation_Estimation.html">22. Robust Rotation and Translation Estimation in Multiview Reconstruction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../FivePoint_Relative_Pose_Problem/FivePoint_Relative_Pose_Problem.html">23. An Effcient Solution to the Five-Point Relative Pose Problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Hybrid_Camera_Estimation/Hybrid_Camera_Estimation.html">24. Hybrid Camera Pose Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Lie_Averaging/Lie_Averaging.html">25. Lie-Algebraic Averaging For Globally Consistent Motion Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Efficient_Robust_Rotation_Averaging/Efficient_Robust_Rotation_Averaging.html">26. Efficient and Robust Large-Scale Rotation Averaging</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Robust_Relative_Rotation_Averaging/Robust_Relative_Rotation_Averaging.html">27. Robust Relative Rotation Averaging</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Privacy_Image_Queries/Privacy_Image_Queries.html">28. Privacy Preserving Image Queries for Camera Localization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Privacy_Location/Privacy_Location.html">29. Privacy Preserving Image-Based Localization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Using%20_Many_Cameras_as_One/Using%20_Many_Cameras_as_One.html">30. Using Many Cameras as One</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Pose_Estimation_From_Points_Lines/Pose_Estimation_From_Points_Lines.html">31. Accurate and Linear Time Pose Estimation from Points and Lines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Solver6L/Solver6L.html">32. Solutions to Minimal Generalized Relative Pose Problems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Geometric_Interpretations/Geometric_Interpretations.html">33. Geometric Interpretations of the Normalized Epipolar Error</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Triangulation_Why_Optimize/Triangulation_Why_Optimize.html">34. Triangulation: Why Optimize?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Planar_Markers/Planar_Markers.html">35. Mapping and Localization from Planar Markers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../GAM/GAM.html">36. Geometry-aware Feature Matching for Structure from Motion Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../RA_MarkerPose/RA_MarkerPose.html">37. Resolving Marker Pose Ambiguity by Robust Rotation Averaging with Clique Constraints</a></li>
<li class="toctree-l2"><a class="reference internal" href="../1DSfM/1DSfM.html">38. Robust Global Translations with 1DSfM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../DeepLearning/SfMLearner/SfMLearner.html">39. Unsupervised Learning of Depth and Ego-Motion from Video</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../DeepLearning/DeepSfM/DeepSfM.html">40. DeepSFM: Structure From Motion Via Deep Bundle Adjustment</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../DeepLearning/Pixel_Perfect_SfM/Pixel_Perfect_SfM.html">41. Pixel-Perfect Structure-from-Motion with Featuremetric Refinement</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../p_slam.html">🍊 SLAM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../p_pointcloud.html">🍋 Point Cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../p_BA.html">🍊 Bundle Adjustment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../p_others.html">🍒 Others</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">源码解析</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../code/colmap.html">🍑 Colmap</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../code/theiasfm.html">🍑 TheiaSfM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../code/ba.html">🍑 Bundle Adjustment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">算法学习</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../algorithm/algorithm.html">🍑 Algorithm</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">工作知识</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../work/work.html">🍑 Work</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">杂七杂八</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../others/o_others.html">🍺 Others</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: black" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">OUCHub</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../../p_sfm.html">🍊 Structure from Motion</a> &raquo;</li>
      <li><span class="section-number">21. </span>A Global Linear Method for Camera Pose Registration</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../_sources/pages/paper/sfm/CameraPose/A_Global_Linear_Method_for_Camera_Pose_Registration/A_Global_Linear_Method_for_Camera_Pose_Registration.rst.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="a-global-linear-method-for-camera-pose-registration">
<h1><span class="section-number">21. </span>A Global Linear Method for Camera Pose Registration<a class="headerlink" href="#a-global-linear-method-for-camera-pose-registration" title="永久链接至标题"></a></h1>
<p>👉 <a class="reference external" href="https://openaccess.thecvf.com/content_iccv_2013/papers/Jiang_A_Global_Linear_2013_ICCV_paper.pdf">原文链接</a></p>
<p>本文提出了一种线性方法，用于从基本矩阵中编码的成对相对位姿中获取全局摄像机姿势。该方法最大程度地减少了近似几何误差，以在相机三元组中强制执行三角关系。</p>
<section id="review">
<h2><span class="section-number">21.1. </span>Review<a class="headerlink" href="#review" title="永久链接至标题"></a></h2>
<p>输入是图像对之间的基本矩阵(五点法计算)。两个图像<span class="math notranslate nohighlight">\(i,j\)</span>之间的基本矩阵Eij提供了相对旋转<span class="math notranslate nohighlight">\(R_{ij}\)</span>和平移方向<span class="math notranslate nohighlight">\(t_{ij}\)</span>。
在此，<span class="math notranslate nohighlight">\(R_{ij}\)</span>是<span class="math notranslate nohighlight">\(3×3\)</span>正交矩阵，而<span class="math notranslate nohighlight">\(t_{ij}\)</span>是<span class="math notranslate nohighlight">\(3×1\)</span>单位向量。</p>
<p>目标是在全局坐标系中恢复所有相机绝对姿势。
使用旋转矩阵<span class="math notranslate nohighlight">\(R_i\)</span>和平移矢量<span class="math notranslate nohighlight">\(c_i\)</span>来表示第<span class="math notranslate nohighlight">\(i\)</span>个相机的方向和位置。(1)</p>
<div class="math notranslate nohighlight">
\[R_j = R_{ij}R_i~~~~~R_j (c_i-c_j) \simeq t_{ij}\]</div>
<p>其中<span class="math notranslate nohighlight">\(\simeq\)</span>表示最大程度相等(equality up to a scale)。</p>
<p>在实际数据中，这些方程将无法精确满足，因此我们需要找到一组最能满足这些方程的<span class="math notranslate nohighlight">\(R_i,c_i\)</span></p>
<p>基于两个标准设计方法：</p>
<p>(1)解决方案应该简单有效。
因为最后要采取BA优化，所以近似的解决方案是可以接受的。</p>
<p>(2)应将摄像机的姿势与场景点分开解决。
场景点通常比摄像机多得多，因此在没有场景点的情况下解决摄像机位姿将大大减少未知数。</p>
<p>首先应用《D. Martinec and T. Pajdla. Robust rotation and translation
estimation in multiview reconstruction. In Proc. CVPR, pages 1–8,
2007.》中描述的线性方法来计算全局摄像机旋转<span class="math notranslate nohighlight">\(R_i\)</span>。它通过忽略<span class="math notranslate nohighlight">\(R_i\)</span>对其列向量的正交约束来过度参数化<span class="math notranslate nohighlight">\(R_i\)</span>，并一次从线性方程<span class="math notranslate nohighlight">\(R_j = R_{ij}R_i\)</span>求解所有旋转矩阵。</p>
<p>固定所有旋转后，我们便可以求解所有相机中心<span class="math notranslate nohighlight">\((c_i,1≤i≤N)\)</span>，而无需重建任何3D点。</p>
</section>
<section id="translation-registration">
<h2><span class="section-number">21.2. </span>Translation Registration<a class="headerlink" href="#translation-registration" title="永久链接至标题"></a></h2>
<p>首先将每一个<span class="math notranslate nohighlight">\(t_{ij}\)</span>通过全局旋转参考帧转换为<span class="math notranslate nohighlight">\(c_{ij} = -R^{T}t_{ij}\)</span>，(1)中对摄像机中心的约束可以写成(2)</p>
<div class="math notranslate nohighlight">
\[c_{ij} \times (c_j - c_i) = 0\]</div>
<p>这是关于未知摄像机中心的线性方程。具有较大基线长度的图像对的方程式具有更大的权重，需要仔细的迭代重加权才能获得良好的结果。</p>
<p>实际上，上述公式使<span class="math notranslate nohighlight">\(c_{ij}\)</span>与基线方向<span class="math notranslate nohighlight">\(c_j -c_i\)</span>之间的叉积最小。下文中导出了一个线性算法，该算法使近似几何误差最小。</p>
<section id="triplet-translation-registration">
<h3><span class="section-number">21.2.1. </span>Triplet Translation Registration<a class="headerlink" href="#triplet-translation-registration" title="永久链接至标题"></a></h3>
<p>从三个摄像机的特殊情况开始。</p>
<p>已知摄像机对之间的相对位移<span class="math notranslate nohighlight">\(c_{ij},c_{ik}\)</span>和<span class="math notranslate nohighlight">\(c_{jk}\)</span>，需要估算相机中心<span class="math notranslate nohighlight">\(c_i,c_j\)</span>和<span class="math notranslate nohighlight">\(c_k\)</span>。理想情况下，三个单位向量<span class="math notranslate nohighlight">\(c_{ij},c_{ik}\)</span>和<span class="math notranslate nohighlight">\(c_{jk}\)</span>应该是共面的。但由于噪声的存在，实际数据中这三个不会共面。</p>
<p>首先考虑<span class="math notranslate nohighlight">\(c_{ij}\)</span>是完美的，并最小化<span class="math notranslate nohighlight">\(c_k\)</span>和两条线<span class="math notranslate nohighlight">\(l(c_i,c_{ik})\)</span>和<span class="math notranslate nohighlight">\(l(c_j,c_{jk})\)</span>的欧几里得距离。</p>
<p>这里，<span class="math notranslate nohighlight">\(l(p,q)\)</span>是经过点p且方向为q的线。由于测量噪声，<span class="math notranslate nohighlight">\(l(c_i,c_{ik})\)</span>和<span class="math notranslate nohighlight">\(l(c_j,c_{jk})\)</span>通常是非共面的。</p>
<p>因此最优解<span class="math notranslate nohighlight">\(c_k\)</span>位于它们共同的垂直线<span class="math notranslate nohighlight">\(AB\)</span>的中点</p>
<figure class="align-center align-default">
<img alt="../../../../../_images/16.png" src="../../../../../_images/16.png" />
</figure>
<p>计算为：(3)</p>
<figure class="align-center align-default">
<img alt="../../../../../_images/24.png" src="../../../../../_images/24.png" />
</figure>
<p>这里<span class="math notranslate nohighlight">\(||c_i - c_j||\)</span>指<span class="math notranslate nohighlight">\(c_i\)</span>和<span class="math notranslate nohighlight">\(c_j\)</span>之间的距离。</p>
<p><span class="math notranslate nohighlight">\(s_{ij}^{ik} = sin(\theta_j')/sin(\theta_k') = ||c_i - c_k|| / ||c_i - c_j||\)</span></p>
<p><span class="math notranslate nohighlight">\(s_{ij}^{jk} = sin(\theta_i')/sin(\theta_k') = ||c_j - c_k|| / ||c_i - c_j||\)</span></p>
<p><span class="math notranslate nohighlight">\(s_{ij}^{ik}\)</span>和<span class="math notranslate nohighlight">\(s_{ij}^{jk}\)</span>是有效基线长度比。角度如上图描述所得(<span class="math notranslate nohighlight">\(\theta_k'\)</span>是线<span class="math notranslate nohighlight">\(c_{ik}\)</span>和<span class="math notranslate nohighlight">\(c_{jk}\)</span>之间的夹角)</p>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>证明：<span class="math notranslate nohighlight">\(c_iA\)</span>和<span class="math notranslate nohighlight">\(c_iB\)</span>的长度大约为<span class="math notranslate nohighlight">\(s_{ij}^{ik}||c_i - c_j||\)</span>和<span class="math notranslate nohighlight">\(s_{ij}^{jk}||c_i - c_j||\)</span></p>
<p>向量<span class="math notranslate nohighlight">\(c_{ij},c_{ik},c_{jk}\)</span>应该接近共面。从而角度<span class="math notranslate nohighlight">\(∠Ac_ic_k\)</span>接近0，长度<span class="math notranslate nohighlight">\(c_iA\)</span>接近<span class="math notranslate nohighlight">\(c_ic_k\)</span></p>
<p>计算<span class="math notranslate nohighlight">\(c_ic_k\)</span>的角度为：</p>
<figure class="align-center align-default">
<img alt="../../../../../_images/32.png" src="../../../../../_images/32.png" />
</figure>
<p>因为三个向量接近共面，所以<span class="math notranslate nohighlight">\(\theta_j' \approx \theta_j,\theta_k' \approx \theta_k\)</span></p>
<p>然后，将<span class="math notranslate nohighlight">\(A\)</span>的3D坐标近似为<span class="math notranslate nohighlight">\(c_i + s_{ij}^{ik}||c_i - c_j|| c_{ik}\)</span></p>
<p>同样的，将<span class="math notranslate nohighlight">\(B\)</span>的3D坐标近似为<span class="math notranslate nohighlight">\(c_j + s_{ij}^{jk}||c_i - c_j|| c_{jk}\)</span></p>
<p>所以，作为AB的中点，<span class="math notranslate nohighlight">\(c_k\)</span>的坐标为…(3)</p>
</div>
<p>等式(3)关于未知相机中心是非线性的。 为了使其线性化：(4)</p>
<figure class="align-center align-default">
<img alt="../../../../../_images/41.png" src="../../../../../_images/41.png" />
</figure>
<p>在此，<span class="math notranslate nohighlight">\(Ri(φ)\)</span>是绕轴<span class="math notranslate nohighlight">\(c_{ij}×c_{ik}\)</span>旋转角度为<span class="math notranslate nohighlight">\(φ\)</span>(逆时针)的矩阵。</p>
<p>从而可以得到线性公式：(5)</p>
<figure class="align-center align-default">
<img alt="../../../../../_images/51.png" src="../../../../../_images/51.png" />
</figure>
<p><span class="math notranslate nohighlight">\(Rj(·)\)</span>是绕方向<span class="math notranslate nohighlight">\(c_{ij}×c_{jk}\)</span>的旋转矩阵。</p>
<p>同样，假设<span class="math notranslate nohighlight">\(c_{ik}\)</span>和<span class="math notranslate nohighlight">\(c_{jk}\)</span>分别没有误差，我们可以得到以下两个摄像机中心线性方程，</p>
<figure class="align-center align-default">
<img alt="../../../../../_images/61.png" src="../../../../../_images/61.png" />
</figure>
<p>解决这三个线性方程可以确定相机的中心。</p>
<p>注意：<strong>公式(5)不需要方向:math:`c_j-c_i`与:math:`c_{ij}`相同。</strong></p>
<p>这会在由相机中心定义的平面中引入旋转模糊度。可以通过在初始配准后计算平均旋转以将<span class="math notranslate nohighlight">\(c_j-c_i,c_k-c_i\)</span>和<span class="math notranslate nohighlight">\(c_k-c_j\)</span>分别与<span class="math notranslate nohighlight">\(c_{ij},c_{ik}\)</span>和<span class="math notranslate nohighlight">\(c_{jk}\)</span>的投影在相机平面上对齐来解决此问题。</p>
</section>
<section id="collinear-camera-motion">
<h3><span class="section-number">21.2.2. </span>Collinear Camera Motion<a class="headerlink" href="#collinear-camera-motion" title="永久链接至标题"></a></h3>
<p>通过正弦角计算基线长度比率仅在<span class="math notranslate nohighlight">\(c_{ij},c_{ik}\)</span>和<span class="math notranslate nohighlight">\(c_{jk}\)</span>不共线时才有效。为了适用所有摄像机类型，本文从局部重建的场景点计算所有基线长度比率。</p>
<p>假设在所有三个图像中可见3D场景点X。从图像<span class="math notranslate nohighlight">\(i,j\)</span>的成对重建中，在假定单位基线长度的情况下，计算图像<span class="math notranslate nohighlight">\(j\)</span>中X的深度<span class="math notranslate nohighlight">\(d^{ij}_j\)</span>。同样可以根据图像<span class="math notranslate nohighlight">\(j,k\)</span>计算图像<span class="math notranslate nohighlight">\(j\)</span>中X的深度为<span class="math notranslate nohighlight">\(d_{j}^{jk}\)</span>。然后将比率<span class="math notranslate nohighlight">\(s_{jk}^{ij}\)</span>估计为<span class="math notranslate nohighlight">\(d_{j}^{jk}/ d_j^{ij}\)</span>。</p>
<p>通常，在所有三个图像中都可以看到多个场景点。</p>
<p>做法是丢弃较远的远点，并使用RANSAC计算平均比率。
注意，只需要局部成对重构即可获得基线长度比。位移配准不涉及在全局坐标系中重建任何场景点。</p>
</section>
<section id="multi-view-translation-registration">
<h3><span class="section-number">21.2.3. </span>Multi-view Translation Registration<a class="headerlink" href="#multi-view-translation-registration" title="永久链接至标题"></a></h3>
<p>该方法可以直接应用于注册多台摄像机。
给定一个三元组图，从其三元组中收集所有方程式(即方程式[5-7])，并求解所得的稀疏线性系统<span class="math notranslate nohighlight">\(Ac = 0\)</span>。</p>
<p><strong>这里，:math:`c`是通过连接所有相机中心形成的向量。
:math:`A`是通过收集所有线性方程形成的矩阵。解是矩阵:math:`A`的非平凡零向量，由与:math:`A^TA`的第四最小特征值相关的特征向量给出。</strong></p>
<p>与三个零特征值关联的特征向量对应于世界坐标系原点的三个自由度。</p>
<p>在所有摄像机共面的特殊情况下，存在与三摄像机情况类似的全局平面内旋转模糊度。可以使用之前描述的相同方法来计算此旋转。</p>
<p>实际上，每个图像都参与不同数量的三元组。
因此，求解<span class="math notranslate nohighlight">\(Ac = 0\)</span>时，将根据包含该特定相机的约束的数量，为未知相机中心隐式赋予不同的权重。</p>
<p>因此，对于每个摄像机<span class="math notranslate nohighlight">\(i\)</span>，计算包含其中心的三元组约束的数量，用<span class="math notranslate nohighlight">\(K_i\)</span>表示。</p>
<p>每个涉及摄像机<span class="math notranslate nohighlight">\(i,j,k\)</span>的三元组约束都由<span class="math notranslate nohighlight">\(\frac{1}{min(K_i,K_j,K_k)}\)</span>重新加权</p>
</section>
</section>
<section id="generalization-to-eg-outliers">
<h2><span class="section-number">21.3. </span>Generalization to EG Outliers<a class="headerlink" href="#generalization-to-eg-outliers" title="永久链接至标题"></a></h2>
<p>如果在成对的对极几何结构(EGs)中没有明显的误差，则可以使用上面两节中描述的方法。</p>
<p>但是，由于可疑的特征匹配，许多图像集(尤其是无序的Internet图像)会生成错误较大的错误EG，尤其是对于具有重复结构的场景。
错误的EG会导致错误估计旋转和平移。</p>
<p>本文采取以下步骤来构建强大的系统：</p>
<ul class="simple">
<li><p>Match Graph Construction：</p></li>
</ul>
<p>对于每个输入图像，我们可以通过词汇树(vocabulary
tree)找到其80个最近的邻居。五点算法可以计算这些图像之间的EG。</p>
<p>然后，构建一个“匹配图”，其中每个图像都是一个顶点，并且如果可以在它们之间计算EG，则连接两个顶点。
仅重建匹配图的最大连接部分。</p>
<ul class="simple">
<li><p>EG Verification：</p></li>
</ul>
<p>执行几个步骤验证以识别不正确的EG：</p>
<ol class="arabic">
<li><p>验证匹配图中的每个三元组，并删除不参与通过验证的三元组的EG。</p>
<p>具体来说，将位移配准应用于每个三元组，并计算配准前后相对位移方向之间的平均差。
如果该平均差大于阈值<span class="math notranslate nohighlight">\(δ_1\)</span>，则认为验证失败。</p>
<p>进一步要求已注册的三重相机可以对至少一个好点(投影误差小于4像素)进行三角测量。</p>
</li>
<li><p>在匹配图的边缘中，提取“可靠边缘”的子集来计算全局摄像机方向。</p>
<p>首先根据边缘的对应数量对每个边缘进行加权，并采用最大的生成树。</p>
<p>然后，遍历所有有效的三元组。
如果三元组的两个边缘在选定的“可靠边缘”集中，插入其第三条边缘。
对该插入进行迭代，以包括尽可能多的可靠边缘。</p>
</li>
<li><p>进一步使用这些相机方向来验证匹配图的边缘，如果循环旋转矩阵《Disambiguating
visual relations using loop
constraints.》与单位矩阵之间的测地距离大于<span class="math notranslate nohighlight">\(δ_2\)</span>，则丢弃边缘。</p>
<p>在这里，循环矩阵为<span class="math notranslate nohighlight">\(R_{ij}^TR_jR_i^T\)</span></p>
</li>
<li><p>只考虑剩余匹配图中最大的连接部分。
通常，将<span class="math notranslate nohighlight">\(δ_1\)</span>和<span class="math notranslate nohighlight">\(δ_2\)</span>分别设置为3°和5°。</p></li>
</ol>
<ul class="simple">
<li><p>Connected Triplet Graph：</p></li>
</ul>
<p>进一步从匹配图中提取连接的三元组图，其中每个三元组都由一个顶点表示。如果两个顶点的三元组在匹配图中具有公共边，则将其连接。</p>
<figure class="align-center align-default">
<img alt="../../../../../_images/71.png" src="../../../../../_images/71.png" />
</figure>
<p>匹配图的单个连接组件可以生成多个连接的三元组图，然后，应用第4节中方法分别计算每个三元组图中的摄像机位置。</p>
<p>在解决了相机位置之后，从feature tracks中对3D场景点进行三角测量。</p>
<p>当存在多个三元组图时，将它们的重建合并以获得最终结果。</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../../LocalMethod/Hybrid/View_Graph_Construction/View_Graph_Construction.html" class="btn btn-neutral float-left" title="20. View-graph construction framework for robust and efficient structure-from-motion" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="../Robust_Rotation_and_Translation_Estimation/Robust_Rotation_and_Translation_Estimation.html" class="btn btn-neutral float-right" title="22. Robust Rotation and Translation Estimation in Multiview Reconstruction" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 2021, linzzz.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>