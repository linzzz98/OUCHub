

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>10. Graph-Based Consistent Matching for Structure-from-Motion &mdash; OUCHub  文档</title>
  

  
  <link rel="stylesheet" href="../../../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../../_static/twemoji.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../../_static/custom.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../../../" src="../../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../../_static/jquery.js"></script>
        <script src="../../../../../../_static/underscore.js"></script>
        <script src="../../../../../../_static/doctools.js"></script>
        <script src="../../../../../../_static/language_data.js"></script>
        <script src="https://twemoji.maxcdn.com/v/latest/twemoji.min.js"></script>
        <script src="../../../../../../_static/twemoji.js"></script>
        <script src="../../../../../../_static/translations.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="索引" href="../../../../../../genindex.html" />
    <link rel="search" title="搜索" href="../../../../../../search.html" />
    <link rel="next" title="11. CSFM: COMMUNITY-BASED STRUCTURE FROM MOTION" href="../../Global/CSfM/CSfM.html" />
    <link rel="prev" title="9. Voting-based Incremental Structure-from-Motion" href="../Voting_based_SfM/Voting_based_SfM.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: black" >
          

          
            <a href="../../../../../../index.html" class="icon icon-home"> OUCHub
          

          
            
            <img src="../../../../../../_static/logo_1.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">基础知识</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../knowledge/k_MH.html">💊 Math</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../knowledge/k_CV.html">🍤 Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../knowledge/k_ML.html">🍎 Machine Learning</a></li>
</ul>
<p class="caption"><span class="caption-text">论文学习</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../../../p_sfm.html">🍊 Structure from Motion</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../Structure-from-Motion_Revisited/Structure-from-Motion_Revisited.html">1. Structure-from-Motion Revisited</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Hierarchical_Structure-And-Motion/Hierarchical_Structure-And-Motion.html">2. Hierarchical structure-and-motion recovery from uncalibrated images</a></li>
<li class="toctree-l2"><a class="reference internal" href="../View-graph_SfM/View-graph_SfM.html">3. View-graph Selection Framework for SfM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../View-graph_Framework/View-graph_Framework.html">4. View-graph construction framework for robust and efficient structure-from-motion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../MarkerSfM/MarkerSfM.html">5. Improved Structure from Motion Using Fiducial Marker Matching</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Privacy_SfM/Privacy_SfM.html">6. Privacy Preserving Structure-from-Motion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../SfM_with_Line_Segments/SfM_with_Line_Segments.html">7. Structure from Motion with Line Segments Under Relaxed Endpoint Constraints</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Line_based_SfM/Line_based_SfM.html">8. Line-based Robust SfM with Little Image Overlap</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Voting_based_SfM/Voting_based_SfM.html">9. Voting-based Incremental Structure-from-Motion</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">10. Graph-Based Consistent Matching for Structure-from-Motion</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#related-work">10.1. Related Work</a></li>
<li class="toctree-l3"><a class="reference internal" href="#problem-formulation">10.2. Problem  Formulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#graph-based-consistent-matching">10.3. Graph-Based Consistent Matching</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#match-graph-initialization">10.3.1. Match Graph Initialization</a></li>
<li class="toctree-l4"><a class="reference internal" href="#graph-expansion-by-strong-triplets">10.3.2. Graph Expansion by Strong Triplets</a></li>
<li class="toctree-l4"><a class="reference internal" href="#community-based-graph-reinforcement">10.3.3. Community-Based Graph Reinforcement</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../Global/CSfM/CSfM.html">11. CSFM: COMMUNITY-BASED STRUCTURE FROM MOTION</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Global/Very_Large-Scale_Global_SfM/Very_Large-Scale_Global_SfM.html">12. Very Large-Scale Global SfM by Distributed Motion Averaging</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Global/GlobalSfM_SA/GlobalSfM_SA.html">13. Global Structure-from-Motion by Similarity Averaging</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Global/GlobalSfM_Application/GlobalSfM_Application.html">14. Global Structure-from-Motion and Its Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Global/Efficient_Initial_Pose-graph/Efficient_Initial_Pose-graph.html">15. Efficient Initial Pose-graph Generation for Global SfM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Global/Optimizing_the_Viewing_Graph/Optimizing_the_Viewing_Graph.html">16. Optimizing the Viewing Graph for Structure-from-Motion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Global/Reducing_Drift_Using_Extended_Feature/Reducing_Drift_Using_Extended_Feature.html">17. Reducing Drift in Structure From Motion Using Extended Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Hybrid/HSfM/HSfM.html">18. HSfM: Hybrid Structure-from-Motion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Hybrid/View_Graph_Construction/View_Graph_Construction.html">19. View-graph construction framework for robust and efficient structure-from-motion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/A_Global_Linear_Method_for_Camera_Pose_Registration/A_Global_Linear_Method_for_Camera_Pose_Registration.html">20. A Global Linear Method for Camera Pose Registration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/Robust_Rotation_and_Translation_Estimation/Robust_Rotation_and_Translation_Estimation.html">21. Robust Rotation and Translation Estimation in Multiview Reconstruction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/FivePoint_Relative_Pose_Problem/FivePoint_Relative_Pose_Problem.html">22. An Effcient Solution to the Five-Point Relative Pose Problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/Hybrid_Camera_Estimation/Hybrid_Camera_Estimation.html">23. Hybrid Camera Pose Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/Lie_Averaging/Lie_Averaging.html">24. Lie-Algebraic Averaging For Globally Consistent Motion Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/Efficient_Robust_Rotation_Averaging/Efficient_Robust_Rotation_Averaging.html">25. Efficient and Robust Large-Scale Rotation Averaging</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/Robust_Relative_Rotation_Averaging/Robust_Relative_Rotation_Averaging.html">26. Robust Relative Rotation Averaging</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/Privacy_Image_Queries/Privacy_Image_Queries.html">27. Privacy Preserving Image Queries for Camera Localization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/Privacy_Location/Privacy_Location.html">28. Privacy Preserving Image-Based Localization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/Using _Many_Cameras_as_One/Using _Many_Cameras_as_One.html">29. Using Many Cameras as One</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/Pose_Estimation_From_Points_Lines/Pose_Estimation_From_Points_Lines.html">30. Accurate and Linear Time Pose Estimation from Points and Lines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/Solver6L/Solver6L.html">31. Solutions to Minimal Generalized Relative Pose Problems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../DeepLearning/SfMLearner/SfMLearner.html">32. Unsupervised Learning of Depth and Ego-Motion from Video</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../DeepLearning/DeepSfM/DeepSfM.html">33. DeepSFM: Structure From Motion Via Deep Bundle Adjustment</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../p_slam.html">🍊 SLAM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../p_pointcloud.html">🍋 Point Cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../p_BA.html">🍊 Bundle Adjustment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../p_others.html">🍒 Others</a></li>
</ul>
<p class="caption"><span class="caption-text">源码解析</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../code/colmap.html">🍑 Colmap</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../code/theiasfm.html">🍑 TheiaSfM</a></li>
</ul>
<p class="caption"><span class="caption-text">杂七杂八</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../others/o_others.html">🍺 Others</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../index.html">OUCHub</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../../p_sfm.html">🍊 Structure from Motion</a> &raquo;</li>
        
      <li><span class="section-number">10. </span>Graph-Based Consistent Matching for Structure-from-Motion</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../../../../_sources/pages/paper/sfm/LocalMethod/Incremental/Graph_based_SfM/Graph_based_SfM.rst.txt" rel="nofollow"> 查看页面源码</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="graph-based-consistent-matching-for-structure-from-motion">
<h1><span class="section-number">10. </span>Graph-Based Consistent Matching for Structure-from-Motion<a class="headerlink" href="#graph-based-consistent-matching-for-structure-from-motion" title="永久链接至标题">¶</a></h1>
<p>本文介绍了一种基于图形的图像匹配方法，它解决了统一框架中的完整性、有效性和一致性问题。
本文的方法从使用基于视觉相似性的最小生成树链接除单例图像之外的所有图像开始。然后对最小生成树进行增量扩展，形成局部一致的强三元组。
最后，引入了一种基于全局社区的图算法，通过增强潜在的大连接组件来增强全局一致性。</p>
<p>图像匹配是三维重建中计算量很大的步骤，尤其是对于大规模无序图像数据集。
由于图像中存在大量的高维特征描述符，朴素的二次匹配方案给大规模高分辨率三维重建带来了沉重的计算负担。
SfM系统通常使用词汇树来选择视觉上相似的匹配对，这相对于图像数量将成对图像匹配的复杂度从 <span class="math notranslate nohighlight">\(0(N^2)\)</span> 降低到 <span class="math notranslate nohighlight">\(0(kn)\)</span> 。</p>
<p>然而，还有两个问题有待解决。</p>
<ol class="arabic simple">
<li><p>图像索引技术的一个主要缺点是查询图像的检索项目 <span class="math notranslate nohighlight">\(k\)</span> 的数量难以确定。许多方案采用了经验相似性阈值或固定检索数，这忽略了图像集合的全局连通性。有时还需要后处理步骤（如查询扩展)，以防止丢失真正的正匹配。</p></li>
<li><p>由于对称和重复的纹理模式，大规模图像数据集经常包含模糊场景。这些重复但不同的图案不仅在视觉上相似，而且可以通过两视图几何验证，形成错误的对极几何。</p></li>
</ol>
<p>因此，高效且一致的匹配子集优于可能包含虚假匹配的冗余匹配集，这与尽可能挖掘连通性的原则有些矛盾。</p>
<p>一致的匹配图对于三维重建的成功至关重要。</p>
<p>在本文中，提出了一种匹配算法，该算法可以高效地生成覆盖整个图像数据集的稀疏匹配图，同时过滤掉通过双视图几何验证的不一致匹配。
该方法同时发现场景的连通性模式，并以一致的方式在计算效率和高效图像连通性之间实现良好的平衡。</p>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>匹配集的一致性是通过在连续的步骤中局部地和全局地实施循环一致性来保证的。</p>
<p>《Zach,  C.,  Klopschitz,  M.,  Pollefeys,  M.:  Disambiguating  visual  relations  using  loop  con- straints.  In: CVPR.  pp.  1426–1433 (2010)》</p>
</div>
<div class="section" id="related-work">
<h2><span class="section-number">10.1. </span>Related Work<a class="headerlink" href="#related-work" title="永久链接至标题">¶</a></h2>
<p>词汇树、结合几何线索、查询扩展、相关反馈和熵最小化、抢占式匹配、基于学习的方法来预测一对图像是否具有重叠区域、基于循环一致性和低秩建模的多图像匹配算法。</p>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>通过匹配图像中的一小部分局部描述符，可以决定是否继续进行完整的假定特征匹配。</p>
</div>
<div class="admonition important">
<p class="admonition-title">重要</p>
<p>在大规模的城市场景中，更多的匹配并不一定能保证更好的重建！！！</p>
</div>
<p>本文专注于匹配图的优化，并在不改变增量或全局 SfM pipeline 的情况下提出一种有效的匹配图构建方法。</p>
</div>
<div class="section" id="problem-formulation">
<h2><span class="section-number">10.2. </span>Problem  Formulation<a class="headerlink" href="#problem-formulation" title="永久链接至标题">¶</a></h2>
<p>输入是一组图像 <span class="math notranslate nohighlight">\(\mathcal{I} = \{I_i\}\)</span> 及其对应的特征点。匹配方法基于对底层图形编码成对匹配和对极几何的分析。</p>
<p>将无向匹配图表示为  <span class="math notranslate nohighlight">\(G = (\mathcal{V},\mathcal{E})\)</span> ，其中每个顶点 <span class="math notranslate nohighlight">\(v_i \in \mathcal{V}\)</span> 对应于图像 <span class="math notranslate nohighlight">\(I_i =\in \mathcal{I}\)</span> 。两个顶点 <span class="math notranslate nohighlight">\(v_i\)</span> 和 <span class="math notranslate nohighlight">\(v_j\)</span> 由边 <span class="math notranslate nohighlight">\(e_{ij} \in \mathcal{E}\)</span> 连接，</p>
<p>如果它们对应图在对极几何验证后具有多个 <span class="math notranslate nohighlight">\(S_I\)</span> 内点。
每个边缘 <span class="math notranslate nohighlight">\(e_{ij}\)</span> 与使用五点算法计算的图像对之间的对极几何和相对运动相关联。
初始边集 <span class="math notranslate nohighlight">\(E\)</span> 为空，目标是逐步构建匹配图。确保该方法中使用的所有图算法的运行时间比图像匹配操作低一个数量级。</p>
<p>令 <span class="math notranslate nohighlight">\(T_{ij}\)</span> 是与边 <span class="math notranslate nohighlight">\(e_{ij}\)</span> 相关联的抽象几何关系，例如 <span class="math notranslate nohighlight">\(T_{ij}\)</span> 可以是根据特征对应计算的相对旋转 <span class="math notranslate nohighlight">\(R_{ij}\)</span> 。</p>
<p>进一步要求这个几何关系可以被链接起来，用 <span class="math notranslate nohighlight">\(◦\)</span> 表示，并且满足  <span class="math notranslate nohighlight">\(T_{ij} ◦ T_{ji} = \mathbb{I}(\forall i,j)\)</span>  其中 <span class="math notranslate nohighlight">\(\mathbb{I}\)</span> 表示 <strong>Identity map</strong> （标识映射 ？ ）。</p>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>首先考虑闭环的最小配置，并对弱一致匹配图给出以下定义：</p>
<p>定义 1.（弱一致性）匹配图 <span class="math notranslate nohighlight">\(G = (\mathcal{V}, \mathcal{E})\)</span> 是弱 <span class="math notranslate nohighlight">\((\epsilon, \mathcal{E})-\)</span> 一致性的，如果任意3长度循环 <span class="math notranslate nohighlight">\((i,j,k)\)</span> 的成对几何关系 <span class="math notranslate nohighlight">\(T_{ij}, T_{jk}, T_{ki}\)</span> 与关于边集 <span class="math notranslate nohighlight">\(\mathcal{E}\)</span> 满足如下循环一致性约束：</p>
<div class="align-center figure align-default">
<img alt="../../../../../../_images/142.jpg" src="../../../../../../_images/142.jpg" />
</div>
<p>其中距离函数  <span class="math notranslate nohighlight">\(d(\hat{T}, \mathbb{I})\)</span> 测量了链式运动 <span class="math notranslate nohighlight">\(\hat{T}\)</span> 和标识映射 ： <span class="math notranslate nohighlight">\(\mathbb{I}\)</span> 之间的差异。</p>
<p>上述定义没有捕捉到一致匹配图的所有本质，因为一些错误的匹配可能只会在更长的循环中表现出来。 因此，通过定义强一致性来完善这个概念：</p>
<p>定义 2.（强一致性）匹配图  <span class="math notranslate nohighlight">\(G = (V, E)\)</span> 是强 <span class="math notranslate nohighlight">\(\epsilon, \mathcal{E}-\)</span> 一致的，如果对于边集 <span class="math notranslate nohighlight">\(\mathcal{E}\)</span> 的任何长度为 <span class="math notranslate nohighlight">\(m\)</span> 的循环 <span class="math notranslate nohighlight">\((n_0,n_1,...,n_{m-1})\)</span> ，则以下条件成立：</p>
<div class="align-center figure align-default">
<img alt="../../../../../../_images/227.jpg" src="../../../../../../_images/227.jpg" />
</div>
<p>其中表示使用 <span class="math notranslate nohighlight">\(◦\)</span> 运算符链接一组几何变换。</p>
<p>为了找到一致的匹配图，需要平衡以下三个性能标准：</p>
<ol class="arabic simple">
<li><p>完整性。 匹配图的跨度应与图像一样多，以保证 3D 模型的完整性。 这个标准对应于最小化 <span class="math notranslate nohighlight">\(G\)</span> 中连接组件的数量。</p></li>
<li><p>效率。 匹配图构建的时间复杂度应取决于图像集合的底层连接模式。</p></li>
<li><p>一致性。 边缘应该是鲁棒的，这意味着它们中的每一个都包含大量的内部特征匹配，并且通过 <span class="math notranslate nohighlight">\(\epsilon\)</span> （越小越好）和 <span class="math notranslate nohighlight">\(|\mathcal{E}|\)</span> （越大越好）来衡量一致。</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>在定义 1 和定义 2 中。这个标准可能与效率相矛盾，因此需要找到一个好的权衡。</p>
</div>
</div>
</div>
<div class="section" id="graph-based-consistent-matching">
<h2><span class="section-number">10.3. </span>Graph-Based Consistent Matching<a class="headerlink" href="#graph-based-consistent-matching" title="永久链接至标题">¶</a></h2>
<p>本文所提出的方法可以分解为下图所示的三个步骤：</p>
<div class="align-center figure align-default">
<img alt="../../../../../../_images/315.jpg" src="../../../../../../_images/315.jpg" />
</div>
<ol class="arabic simple">
<li><p>匹配图初始化</p></li>
<li><p>强三元组的图扩展</p></li>
<li><p>基于社区的图增强</p></li>
</ol>
<p><strong>完整性：</strong> 匹配图初始化的目的是最小化连接组件的数量并丢弃匹配图中的单例图像。</p>
<p><strong>效率：</strong> 依次应用扩展和强化步骤来有效探索场景结构。</p>
<p><strong>一致性：</strong> 弱一致性和强一致性在此过程中迭代验证。</p>
<div class="section" id="match-graph-initialization">
<h3><span class="section-number">10.3.1. </span>Match Graph Initialization<a class="headerlink" href="#match-graph-initialization" title="永久链接至标题">¶</a></h3>
<p>标准 1 可以通过快速链接图像集合中的视图来单独完成。词汇树给出的相似性分数和排名信息参数化了先验匹配图。修改 Kruskal 的算法，以获得正在进行的匹配图的最小生成树算法。</p>
<div class="align-center figure align-default">
<img alt="../../../../../../_images/414.jpg" src="../../../../../../_images/414.jpg" />
</div>
<p>如果图像集合包含单例视图或分离的场景，初始化过程可能会过长，因为它需要探索每一个可能的边来加入单例图像。
为了增加树结构的稳定性并处理单例图像，考虑相互连接的边缘权重。
相对于数据集中的其他图像查询第 <span class="math notranslate nohighlight">\(i\)</span> 个图像并获得排名列表 <span class="math notranslate nohighlight">\(Rank_i\)</span> 。 图像 <span class="math notranslate nohighlight">\(j\)</span> 在 <span class="math notranslate nohighlight">\(Rank_i\)</span> 中的等级表示为 <span class="math notranslate nohighlight">\(Rank_i(j)\)</span> 。</p>
<dl class="simple">
<dt>节点 <span class="math notranslate nohighlight">\(i\)</span> 和节点 <span class="math notranslate nohighlight">\(j\)</span> 的边权重 <span class="math notranslate nohighlight">\(w(e_{ij})\)</span> 定义 <span class="math notranslate nohighlight">\(Rank_i(j)\)</span> 和 <span class="math notranslate nohighlight">\(Rank_j(i)\)</span> 的二次均值，即</dt><dd><p><span class="math notranslate nohighlight">\(w(e_{ij}) = \sqrt{\frac{Rank_i^2(j) + Rank_j^2(i)}{2}}\)</span></p>
</dd>
</dl>
<p>该算法首先按权重按递增顺序对边集进行排序，然后使用联合查找数据结构探测（特征对应和几何验证）可以连接两个不相交集的最可能对。
如果成功，则合并两个分离的集合； 否则，它将继续连接两个组件的下一个最佳可能边。
如果图像已参与 <span class="math notranslate nohighlight">\(S_R\)</span> 失败测试，则将其视为单例图像并从数据集中丢弃。</p>
<p>树匹配图不包含用于一致性检查的循环。为了获得最稳健的初始匹配对，在匹配验证中应用了更严格的内点阈值 <span class="math notranslate nohighlight">\(S_I^*(=40)\)</span></p>
</div>
<div class="section" id="graph-expansion-by-strong-triplets">
<h3><span class="section-number">10.3.2. </span>Graph Expansion by Strong Triplets<a class="headerlink" href="#graph-expansion-by-strong-triplets" title="永久链接至标题">¶</a></h3>
<p>本文目标是精确地满足匹配图的弱一致性，并近似地保证该图上的强一致性。</p>
<p>用生成树连接不同的视图后，得到一个弱连接的匹配图。有一个公共顶点的两条相邻边形成一个弱三元组，一旦两个端点相连，它就成为一个强三元组。</p>
<div class="align-center figure align-default">
<img alt="../../../../../../_images/512.jpg" src="../../../../../../_images/512.jpg" />
</div>
</div>
<div class="section" id="community-based-graph-reinforcement">
<h3><span class="section-number">10.3.3. </span>Community-Based Graph Reinforcement<a class="headerlink" href="#community-based-graph-reinforcement" title="永久链接至标题">¶</a></h3>
<p>扩展的匹配图稳健地估计场景的局部结构，并生成由强三元组实施的一致匹配集。</p>
<p>尽管这种简化的匹配图有助于生成一致的重构，但它有两个主要缺点。</p>
<ol class="arabic simple">
<li><p>在这个匹配图中，只有强三元组得到验证，较长循环的一致性(强一致性)被忽略。</p></li>
<li><p>该匹配图没有全局范围的闭环结构，不能反映数据集的真实位姿图。</p></li>
</ol>
<p>为了解决上述缺点，本文提出了一种受社区检测技术启发的组件合并算法。</p>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>社区检测广泛用于复杂网络的分析。它旨在将一个图分成内部连接更密集、外部连接更稀疏的组。</p>
</div>
<p>令 <span class="math notranslate nohighlight">\(A_{ij}\)</span> 是一般图的相邻矩阵的一个元素，如果 <span class="math notranslate nohighlight">\(i\)</span> 与 <span class="math notranslate nohighlight">\(j\)</span> 相连，则 <span class="math notranslate nohighlight">\(A_{ij} = 1\)</span> ，否则 <span class="math notranslate nohighlight">\(A_{ij} = 0\)</span> 。</p>
<p>节点 <span class="math notranslate nohighlight">\(i\)</span> 的度 <span class="math notranslate nohighlight">\(d_i\)</span> 是连接到它的其他节点的数量，表示为 <span class="math notranslate nohighlight">\(d_i = \sum\limits_j A_{ij}\)</span> 。</p>
<p>如果图是没有明显社区结构的随机图，则在节点 <span class="math notranslate nohighlight">\(i\)</span> 和节点 <span class="math notranslate nohighlight">\(j\)</span> 之间存在边的概率为 <span class="math notranslate nohighlight">\(\frac{d_i d_j}{2m}\)</span> ，其中 <span class="math notranslate nohighlight">\(m = \frac{1}{2}\sum\limits_{ij} A_{ij}\)</span> 是图 <span class="math notranslate nohighlight">\(G\)</span> 的总边数。</p>
<p>假设匹配图的结构使得节点 <span class="math notranslate nohighlight">\(i\)</span> 属于社区 <span class="math notranslate nohighlight">\(V_p\)</span> ，节点 <span class="math notranslate nohighlight">\(j\)</span> 属于社区 <span class="math notranslate nohighlight">\(V_q\)</span> ，
那么模块性 <span class="math notranslate nohighlight">\(Q\)</span> 测量了图和随机图之间的社区内连接的分数之差:</p>
<div class="math notranslate nohighlight">
\[Q = \frac{1}{2m} \sum\limits_{ij} (A_{ij} - \frac{d_i d_j}{2m}) \delta (V_p, V_q)\]</div>
<p>其中，如果 <span class="math notranslate nohighlight">\(i = j\)</span> ，则 <span class="math notranslate nohighlight">\(\delta(i, j) = 1\)</span> ，否则为0。如果每个节点本身都是一个社区，模块化程度为零。</p>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>实际上，大于0.3的值表示该图具有显著的群体结构。</p>
</div>
<p>为了避免破坏加快成对匹配的目的，选择快速贪婪方法来估计社区结构。
这种分层算法从每个节点作为一个唯一的社区开始，并迭代地加入不同的社区，
这些社区的合并导致 <span class="math notranslate nohighlight">\(Q\)</span> 的最大增加。</p>
<p>具体来说，使用加权图，边缘权重是图像对之间的基本矩阵插值器的数量，
这有助于识别弱连接。因此，匹配图的 <span class="math notranslate nohighlight">\(A_{ij}\)</span> 定义为：</p>
<div class="math notranslate nohighlight">
\[\begin{split}A_{ij} = \begin{cases}
F_{inlier} ~i ~and~ j~are~connected\\
0~~~~~~~~otherwise
\end{cases}\end{split}\]</div>
<p>在 <span class="math notranslate nohighlight">\(n - 1\)</span> 个这样的连接之后，匹配图被合并成单个社区。</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="../../Global/CSfM/CSfM.html" class="btn btn-neutral float-right" title="11. CSFM: COMMUNITY-BASED STRUCTURE FROM MOTION" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="../Voting_based_SfM/Voting_based_SfM.html" class="btn btn-neutral float-left" title="9. Voting-based Incremental Structure-from-Motion" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; 版权所有 2021, linzzz.

    </p>
  </div> 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>