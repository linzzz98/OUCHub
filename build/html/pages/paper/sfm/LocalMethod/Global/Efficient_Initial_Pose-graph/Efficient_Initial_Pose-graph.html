

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>15. Efficient Initial Pose-graph Generation for Global SfM &mdash; OUCHub  文档</title>
  

  
  <link rel="stylesheet" href="../../../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../../_static/twemoji.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../../_static/custom.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../../../" src="../../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../../_static/jquery.js"></script>
        <script src="../../../../../../_static/underscore.js"></script>
        <script src="../../../../../../_static/doctools.js"></script>
        <script src="../../../../../../_static/language_data.js"></script>
        <script src="https://twemoji.maxcdn.com/v/latest/twemoji.min.js"></script>
        <script src="../../../../../../_static/twemoji.js"></script>
        <script src="../../../../../../_static/translations.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="索引" href="../../../../../../genindex.html" />
    <link rel="search" title="搜索" href="../../../../../../search.html" />
    <link rel="next" title="16. Optimizing the Viewing Graph for Structure-from-Motion" href="../Optimizing_the_Viewing_Graph/Optimizing_the_Viewing_Graph.html" />
    <link rel="prev" title="14. Global Structure-from-Motion and Its Application" href="../GlobalSfM_Application/GlobalSfM_Application.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: black" >
          

          
            <a href="../../../../../../index.html" class="icon icon-home"> OUCHub
          

          
            
            <img src="../../../../../../_static/logo_1.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">基础知识</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../knowledge/k_MH.html">💊 Math</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../knowledge/k_CV.html">🍤 Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../knowledge/k_ML.html">🍎 Machine Learning</a></li>
</ul>
<p class="caption"><span class="caption-text">论文学习</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../../../p_sfm.html">🍊 Structure from Motion</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../Incremental/Structure-from-Motion_Revisited/Structure-from-Motion_Revisited.html">1. Structure-from-Motion Revisited</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Incremental/Hierarchical_Structure-And-Motion/Hierarchical_Structure-And-Motion.html">2. Hierarchical structure-and-motion recovery from uncalibrated images</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Incremental/View-graph_SfM/View-graph_SfM.html">3. View-graph Selection Framework for SfM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Incremental/View-graph_Framework/View-graph_Framework.html">4. View-graph construction framework for robust and efficient structure-from-motion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Incremental/MarkerSfM/MarkerSfM.html">5. Improved Structure from Motion Using Fiducial Marker Matching</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Incremental/Privacy_SfM/Privacy_SfM.html">6. Privacy Preserving Structure-from-Motion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Incremental/SfM_with_Line_Segments/SfM_with_Line_Segments.html">7. Structure from Motion with Line Segments Under Relaxed Endpoint Constraints</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Incremental/Line_based_SfM/Line_based_SfM.html">8. Line-based Robust SfM with Little Image Overlap</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Incremental/Voting_based_SfM/Voting_based_SfM.html">9. Voting-based Incremental Structure-from-Motion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Incremental/Graph_based_SfM/Graph_based_SfM.html">10. Graph-Based Consistent Matching for Structure-from-Motion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../CSfM/CSfM.html">11. CSFM: COMMUNITY-BASED STRUCTURE FROM MOTION</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Very_Large-Scale_Global_SfM/Very_Large-Scale_Global_SfM.html">12. Very Large-Scale Global SfM by Distributed Motion Averaging</a></li>
<li class="toctree-l2"><a class="reference internal" href="../GlobalSfM_SA/GlobalSfM_SA.html">13. Global Structure-from-Motion by Similarity Averaging</a></li>
<li class="toctree-l2"><a class="reference internal" href="../GlobalSfM_Application/GlobalSfM_Application.html">14. Global Structure-from-Motion and Its Application</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">15. Efficient Initial Pose-graph Generation for Global SfM</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#abstract">15.1. Abstract</a></li>
<li class="toctree-l3"><a class="reference internal" href="#introduction">15.2. Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#related-tech">15.2.1. Related tech</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#relative-pose-from-directed-walks">15.3. Relative Pose from Directed Walks</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pose-graph-traversal">15.4. Pose-graph Traversal</a></li>
<li class="toctree-l3"><a class="reference internal" href="#guided-matching-with-pose">15.5. Guided  Matching  with  Pose</a></li>
<li class="toctree-l3"><a class="reference internal" href="#adaptive-correspondence-ranking">15.6. Adaptive Correspondence Ranking</a></li>
<li class="toctree-l3"><a class="reference internal" href="#experiments">15.7. Experiments</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../Optimizing_the_Viewing_Graph/Optimizing_the_Viewing_Graph.html">16. Optimizing the Viewing Graph for Structure-from-Motion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Reducing_Drift_Using_Extended_Feature/Reducing_Drift_Using_Extended_Feature.html">17. Reducing Drift in Structure From Motion Using Extended Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Hybrid/HSfM/HSfM.html">18. HSfM: Hybrid Structure-from-Motion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Hybrid/View_Graph_Construction/View_Graph_Construction.html">19. View-graph construction framework for robust and efficient structure-from-motion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/A_Global_Linear_Method_for_Camera_Pose_Registration/A_Global_Linear_Method_for_Camera_Pose_Registration.html">20. A Global Linear Method for Camera Pose Registration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/Robust_Rotation_and_Translation_Estimation/Robust_Rotation_and_Translation_Estimation.html">21. Robust Rotation and Translation Estimation in Multiview Reconstruction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/FivePoint_Relative_Pose_Problem/FivePoint_Relative_Pose_Problem.html">22. An Effcient Solution to the Five-Point Relative Pose Problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/Hybrid_Camera_Estimation/Hybrid_Camera_Estimation.html">23. Hybrid Camera Pose Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/Lie_Averaging/Lie_Averaging.html">24. Lie-Algebraic Averaging For Globally Consistent Motion Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/Efficient_Robust_Rotation_Averaging/Efficient_Robust_Rotation_Averaging.html">25. Efficient and Robust Large-Scale Rotation Averaging</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/Robust_Relative_Rotation_Averaging/Robust_Relative_Rotation_Averaging.html">26. Robust Relative Rotation Averaging</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/Privacy_Image_Queries/Privacy_Image_Queries.html">27. Privacy Preserving Image Queries for Camera Localization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/Privacy_Location/Privacy_Location.html">28. Privacy Preserving Image-Based Localization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/Using _Many_Cameras_as_One/Using _Many_Cameras_as_One.html">29. Using Many Cameras as One</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/Pose_Estimation_From_Points_Lines/Pose_Estimation_From_Points_Lines.html">30. Accurate and Linear Time Pose Estimation from Points and Lines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/Solver6L/Solver6L.html">31. Solutions to Minimal Generalized Relative Pose Problems</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../DeepLearning/SfMLearner/SfMLearner.html">32. Unsupervised Learning of Depth and Ego-Motion from Video</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../DeepLearning/DeepSfM/DeepSfM.html">33. DeepSFM: Structure From Motion Via Deep Bundle Adjustment</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../p_slam.html">🍊 SLAM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../p_pointcloud.html">🍋 Point Cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../p_others.html">🍒 Others</a></li>
</ul>
<p class="caption"><span class="caption-text">源码解析</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../code/colmap.html">🍑 Colmap</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../code/theiasfm.html">🍑 TheiaSfM</a></li>
</ul>
<p class="caption"><span class="caption-text">杂七杂八</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../others/o_others.html">🍺 Others</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../index.html">OUCHub</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../../p_sfm.html">🍊 Structure from Motion</a> &raquo;</li>
        
      <li><span class="section-number">15. </span>Efficient Initial Pose-graph Generation for Global SfM</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../../../../_sources/pages/paper/sfm/LocalMethod/Global/Efficient_Initial_Pose-graph/Efficient_Initial_Pose-graph.rst.txt" rel="nofollow"> 查看页面源码</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="efficient-initial-pose-graph-generation-for-global-sfm">
<h1><span class="section-number">15. </span>Efficient Initial Pose-graph Generation for Global SfM<a class="headerlink" href="#efficient-initial-pose-graph-generation-for-global-sfm" title="永久链接至标题">¶</a></h1>
<div class="section" id="abstract">
<h2><span class="section-number">15.1. </span>Abstract<a class="headerlink" href="#abstract" title="永久链接至标题">¶</a></h2>
<p>文章提出了加速全局SfM算法的初始位姿图生成的方法。</p>
<p>位姿图创建中最耗时的步骤是由 FLANN 形成暂时的特征点对应关系（不一定正确的对应关系）和由 RANSAC 进行几何验证，为了避免这个步骤，文章提出了两种新方法——基于图像对（通常连续匹配）。
候选的相对位姿可以从部分构建的位姿图的路径中恢复。</p>
<p>考虑到图像的全局相似性和位姿图边缘的质量， 作者提出了 A* 遍历的启发式方法。给定来自路径的相对姿态，通过利用已知的对极几何，基于描述符的特征匹配变得“轻量级”。</p>
<p>为了在应用 RANSAC 时加快基于 PROSAC 的采样，文章提出了第三种方法，根据先前估计的内点概率对对应项进行排序。</p>
</div>
<div class="section" id="introduction">
<h2><span class="section-number">15.2. </span>Introduction<a class="headerlink" href="#introduction" title="永久链接至标题">¶</a></h2>
<div class="align-center figure align-default" id="id1">
<img alt="../../../../../../_images/127.jpg" src="../../../../../../_images/127.jpg" />
<p class="caption"><span class="caption-text">global sfm pipeline</span><a class="headerlink" href="#id1" title="永久链接至图片">¶</a></p>
</div>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>目前的技术都没有考虑到匹配是在多个图像对上执行的，其中在匹配之前，相对位姿可能是已知的，至少大约是已知的。</p>
</div>
<dl class="field-list">
<dt class="field-odd">过程</dt>
<dd class="field-odd"><ol class="arabic simple">
<li><p>在所有图像中提取特征。这个步骤很容易并行化，并且具有O(n)个时间复杂度，其中n是重建中要包含的图像的数量。</p></li>
<li><p>这些特征通常用于将图像对从最可能匹配到最难匹配的图像对排序，例如，通过视觉词袋。</p></li>
<li><p>通过匹配检测到的特征点的通常高维（例如，128维度的SIFT）描述符，在所有图像对之间产生暂时的点对应关系。</p></li>
<li><p>通过应用RANSAC来过滤对应关系并估计所有图像对之间的相对姿态。</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>通常，特征匹配和几何估计步骤是迄今为止最慢的部分，两者都对图像的数量具有二次复杂度。此外，特征匹配具有二次最坏情况的时间复杂度，因为它取决于各自图像中特征数的乘积。</p>
</div>
<ol class="arabic simple" start="5">
<li><p>全局调整（BA）得到了成对位姿的精确重构。与初始的位姿图生成相比，这个步骤有可以忽略不计的时间需求。</p></li>
</ol>
</dd>
<dt class="field-even">本文三个主要贡献</dt>
<dd class="field-even"><ol class="arabic simple">
<li><p>提出了一种建立部分位姿图来避免基于计算要求的基于RANSAC的鲁棒估计的方法。</p></li>
<li><p>提出了一种通过使用由 <strong>A∗算法</strong> 确定的位姿的技术。这种引导匹配方法使用本质矩阵有效地选择关键点。</p></li>
<li><p>提出了一种根据历史点对点的对应关系进行自适应重新排序的算法。</p></li>
</ol>
</dd>
</dl>
<div class="section" id="related-tech">
<h3><span class="section-number">15.2.1. </span>Related tech<a class="headerlink" href="#related-tech" title="永久链接至标题">¶</a></h3>
<p>使用以下方法生成完全连接的图像相似度图：</p>
<blockquote>
<div><p>使用 ResNet-50 CNN 提取 GeM 图像全局描述符，在 GLD-v1 数据集上进行预训练。然后计算所有描述符之间的内积相似度，得到一个 n × n 相似度矩阵。</p>
</div></blockquote>
</div>
</div>
<div class="section" id="relative-pose-from-directed-walks">
<h2><span class="section-number">15.3. </span>Relative Pose from Directed Walks<a class="headerlink" href="#relative-pose-from-directed-walks" title="永久链接至标题">¶</a></h2>
<p>本文提出了一种通过尽可能 <strong>避免运行 RANSAC</strong> 来加速位姿图生成的方法。</p>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>核心思想利用了这样一个事实：在从图像集合中估计第 (t+1) 个图像对之间的相对位姿时，会得到一个由 t 个边（即 t 个视图对）组成的位姿图。该位姿图通常可用于估计位姿，而无需运行类似 RANSAC 的稳健估计。</p>
</div>
<p>在接下来的描述中，假设视图对是按照它们的相似度得分排序的。因此，从最相似的视图对开始姿态估计。让我们假设我们已经成功匹配了 t 个图像对，因此得到了位姿图： <span class="math notranslate nohighlight">\(\mathcal{G}_t = \{V,E\}\)</span></p>
<div class="align-center figure align-default">
<img alt="../../../../../../_images/218.jpg" src="../../../../../../_images/218.jpg" />
</div>
<p>在估计第 (t + 1) 个视图对之间的相对位姿时有两个选择。</p>
<p>传统的方法是对两幅图像之间的对应点进行鲁棒估计。然后将估计的位姿  <span class="math notranslate nohighlight">\(P ∈ SE(3)\)</span>  添加到位姿图中作为新边的位姿。 因此， <span class="math notranslate nohighlight">\(\mathcal{E}_{t+1} = \mathcal{E}_t ∪ \{e = (v_s, v_d)\}\)</span> ，并且 <span class="math notranslate nohighlight">\(\phi(e) = P\)</span>
这一步的问题在于，当内点很少，因此内点比率较低时，估计通常很耗时。</p>
<div class="admonition important">
<p class="admonition-title">重要</p>
<p>本文建议使用先前生成的姿势图  <span class="math notranslate nohighlight">\(\mathcal{G}_t\)</span> ，而不是在一对视图  <span class="math notranslate nohighlight">\((v_s, v_d)\)</span> 之间盲目地估计位姿。</p>
</div>
<div class="align-center figure align-default" id="id2">
<img alt="../../../../../../_images/311.jpg" src="../../../../../../_images/311.jpg" />
<p class="caption"><span class="caption-text">文章使用的所有符号</span><a class="headerlink" href="#id2" title="永久链接至图片">¶</a></p>
</div>
<div class="align-center figure align-default">
<img alt="../../../../../../_images/49.jpg" src="../../../../../../_images/49.jpg" />
</div>
<p>将  <span class="math notranslate nohighlight">\(\mathcal{W}\)</span> 隐含的位姿递归地定义为：</p>
<div class="align-center figure align-default">
<img alt="../../../../../../_images/57.jpg" src="../../../../../../_images/57.jpg" />
</div>
<p>因此，给定有限的步行  <span class="math notranslate nohighlight">\(\mathcal{W}\)</span> ，视图  <span class="math notranslate nohighlight">\(v_s\)</span> 和 <span class="math notranslate nohighlight">\(v_d\)</span> 之间的相对位姿计算为 <span class="math notranslate nohighlight">\(\phi(\mathcal{W})\)</span> 。</p>
<div class="align-center figure align-default">
<img alt="../../../../../../_images/66.jpg" src="../../../../../../_images/66.jpg" />
</div>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>简单来说就是估计两视图之间的位姿，不是靠两视图几何估计，而是通过已有的位姿图，进行迭代式估计（s-&gt;t 经过了 1， 2， 3， 则先估计s-&gt;1-&gt;2-&gt;3-&gt;t）</p>
<p>问题：这样不会导致误差累计吗？</p>
</div>
<p>上面公式的问题在于单个错误估计的位姿，使整个 <span class="math notranslate nohighlight">\(\phi(\mathcal{W})\)</span> 错误。因此，需要在给定距离内找到多次walk，即限制最大深度以避免无限长的walk，返回的步行按照顺序立刻评估：</p>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>如果其中有一个视图的位姿是错误的，则会导致整体位姿估计失败，因此需要多条路径同时估计，互相约束。</p>
</div>
<p>每当发现一个新的walk <span class="math notranslate nohighlight">\(\mathcal{W}\)</span> 时，从位姿  <span class="math notranslate nohighlight">\(\phi(\mathcal{W})\)</span> 以及源图像和目标图像之间的对应关系 <span class="math notranslate nohighlight">\(v_s\)</span> 和 <span class="math notranslate nohighlight">\(v_d\)</span> 分别计算出它的inlier ratio。</p>
<dl class="field-list">
<dt class="field-odd">Termination</dt>
<dd class="field-odd"><p>有两种情况下 寻找和测试行的程序终止。</p>
<ol class="arabic simple">
<li><p>当在最大距离内找不到更多walk时，该过程结束。</p></li>
<li><p>如果找到了相当好的位姿 <span class="math notranslate nohighlight">\(P\)</span> ，则过程终止。 如果至少具有  <span class="math notranslate nohighlight">\(I_{min}\)</span> 个内点，则认为该相对位姿是好的。</p></li>
</ol>
</dd>
<dt class="field-even">Pose refinement</dt>
<dd class="field-even"><p>如果从其中一次walk中成功获得位姿，则仅从位姿图 <span class="math notranslate nohighlight">\(G_t\)</span> 的边缘计算，而不考虑图像 <span class="math notranslate nohighlight">\(v_s\)</span> 和 <span class="math notranslate nohighlight">\(v_d\)</span> 之间的对应关系。</p>
<p>为了提高精度并获得 <span class="math notranslate nohighlight">\(P^{*}\)</span> ，应用由新估计模型 <span class="math notranslate nohighlight">\(P\)</span> 初始化的迭代重新加权最小二乘拟合。</p>
<div class="math notranslate nohighlight">
\[\mathcal{E}_{t+1} = \mathcal{E}_t \cup \{e = (v_s,v_d)\}\]</div>
<p>并且 <span class="math notranslate nohighlight">\(\phi(e) = P^{*}\)</span></p>
</dd>
<dt class="field-odd">Failures</dt>
<dd class="field-odd"><p>在某些情况下，视图 <span class="math notranslate nohighlight">\(v_s\)</span> 和 <span class="math notranslate nohighlight">\(v_d\)</span> 之间至少存在一次walk，但隐含的位姿是不正确的，即它不会得到足够多的内点。 在这些情况下，应用传统方法，即基于 RANSAC 的鲁棒估计。</p>
</dd>
<dt class="field-even">Visibility</dt>
<dd class="field-even"><p>可以通过联合查找算法在 <span class="math notranslate nohighlight">\(O(1)\)</span> 时间内确定在视图 <span class="math notranslate nohighlight">\(v_s\)</span> 和 <span class="math notranslate nohighlight">\(v_d\)</span> 之间的位姿图中是否至少有一次walk。</p>
</dd>
</dl>
</div>
<div class="section" id="pose-graph-traversal">
<h2><span class="section-number">15.4. </span>Pose-graph Traversal<a class="headerlink" href="#pose-graph-traversal" title="永久链接至标题">¶</a></h2>
<p>本文选择 <span class="math notranslate nohighlight">\(A^*\)</span> 算法的启发式方法来获得位姿图 <span class="math notranslate nohighlight">\(\mathcal{G}\)</span> 中 multiple walks 的方法。</p>
<dl class="field-list simple">
<dt class="field-odd">目标</dt>
<dd class="field-odd"><p>制定一个启发式算法，引导A∗算法从节点 <span class="math notranslate nohighlight">\(v_s\)</span> 到节点 <span class="math notranslate nohighlight">\(v_d\)</span> ，同时尽可能少地访问顶点。</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>由于得到的是一个相对位姿图，所以无法确定一个度量去衡量一个视图对的欧氏距离。</p>
<p>当有相对位姿时，全局和局部的尺度都是未知的，因此，所有的位移都是单位长度。</p>
</div>
<p>视图 <span class="math notranslate nohighlight">\(v_s\)</span> 和 <span class="math notranslate nohighlight">\(v_d\)</span> 的全局相似性衡量为 <span class="math notranslate nohighlight">\(\delta(v_s, v_d), \delta : V \times V -&gt; R\)</span> 。</p>
<p>全局相似性是通过GeM描述符与ResNet-50 CNN的内积确定的，一条估计不正确的边会严重影响整个walk的位姿，通过函数 <span class="math notranslate nohighlight">\(\rho(e) : E -&gt; R\)</span> 来考虑边缘 <span class="math notranslate nohighlight">\(e\)</span> 的质量。函数 <span class="math notranslate nohighlight">\(\rho(e)\)</span> 返回
给定的当前边缘的位姿 <span class="math notranslate nohighlight">\(\phi(e)\)</span> 和点对应关系所计算的inlier比率。</p>
<p>一个不正确的姿势使 <span class="math notranslate nohighlight">\(\phi(W)\)</span> 。因此， <span class="math notranslate nohighlight">\(W\)</span> 的质量被测量为 <span class="math notranslate nohighlight">\(Q(W) = min_{f \in w} \rho(f)\)</span> ，即最不准确的边的质量。</p>
<p>为了衡量目的地视图 <span class="math notranslate nohighlight">\(v_d\)</span> 之间的walk的相似性，定义函数：</p>
<div class="math notranslate nohighlight">
\[\Delta (W, v_d) = \max_{f=(v_1,v_2)\in W} \delta (v_2,d_2)\]</div>
<p>例如：最相似的顶点决定了相似度。</p>
<p>考虑到walk的质量和与目的地的相似性的启发式方法为：</p>
<div class="math notranslate nohighlight">
\[h(W) = \lambda \mathop{min}_{f\in W} \rho(f) + (1 - \lambda) \mathop{max}_{f = (v_1, v_2) \in W} \delta(v_2, d_2)\]</div>
<dl class="field-list">
<dt class="field-odd">其中</dt>
<dd class="field-odd"><p><span class="math notranslate nohighlight">\(\lambda \in [0,1]\)</span> 是权重参数，</p>
<p><span class="math notranslate nohighlight">\(min_{f∈W} \rho(f)\)</span> 使A∗算法找到一个沿途最小的inlier比率的walk。</p>
<p><span class="math notranslate nohighlight">\(max_{f=(v1,v2)∈W} \delta(v_2, v_d)\)</span> 影响图遍历的方式，使其与路径上的目标视图的最大相似度达到最大。</p>
</dd>
</dl>
</div>
<div class="section" id="guided-matching-with-pose">
<h2><span class="section-number">15.5. </span>Guided  Matching  with  Pose<a class="headerlink" href="#guided-matching-with-pose" title="永久链接至标题">¶</a></h2>
<p>加速特征匹配的常用方法是使用近似的近邻搜索，而不是精确的近邻搜索，例如使用FLANN中的kd-tree算法。</p>
<p>本文提出了一个替代性的解决方案–利用来自当前位姿图中walk的位姿来建立暂定的关联。
这些位姿将被用于使标准描述符匹配 “light-weight”，只检查那些与位姿一致的对应关系。</p>
<div class="align-center figure align-default">
<img alt="../../../../../../_images/73.jpg" src="../../../../../../_images/73.jpg" />
</div>
<p>给定第 <span class="math notranslate nohighlight">\(i,j\)</span> 张图像的关键点 <span class="math notranslate nohighlight">\(K_i, K_j\)</span> ，相对位姿 <span class="math notranslate nohighlight">\(P_{i,j} = (t_{i,j}, R_{i,j}) \in SE(3)\)</span> ，可以计算得到本质矩阵 <span class="math notranslate nohighlight">\(E_{ij} = [t_{ij}]_x R_{i,j}\)</span> ，
使用它去测量点对之间的距离 <span class="math notranslate nohighlight">\(\epsilon\)</span> （辛普森距离和对称极线误差）</p>
<p>目的是找到成对点 <span class="math notranslate nohighlight">\((p_i, p_j)，p_i \in K_i, p_j \in K_j\)</span> ，并且 <span class="math notranslate nohighlight">\(\epsilon(p_i, p_j, E_{ij})\)</span> 小于inlier-outlier阈值。</p>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>传统方法特征匹配是在所有可能的关键点的高维描述符向量上使用L2准则来定义。</p>
<p>本文提出的方法使用本质矩阵选择一个小的候选匹配子集。因此速度更快。</p>
</div>
<p>由于在二维进行匹配，该程序可以通过散列来完成。</p>
<p>使用本质矩阵，在源图像 <span class="math notranslate nohighlight">\(v_s\)</span> 中查找可能的点对退化为 <strong>在目标点中查找点</strong> ，其中相应的极线投影到正确的位置，即到源图像 <span class="math notranslate nohighlight">\(v_s\)</span> 中的选定点上。</p>
<p>因此，目标图像 <span class="math notranslate nohighlight">\(v_d\)</span> 中的点可以根据它们在源图像中的极线放入（bins）中。</p>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>什么是 <strong>bin</strong> ?</p>
<p>考虑灰度图像的颜色直方图。每个像素的取值范围为 0 - 255。为了创建直方图，我们计算强度为“I”的像素数量并将 H(I) 指定为计数，其中 H 是直方图。这里，bin 指的是每个强度值“I”。我们有 255 个强度值，因此有 255 个 bin。</p>
<p>或者，我们可以将直方图和聚类强度值简化为组。例如，假设所有强度小于 125 的像素进入一组，其余像素进入第二组。现在，我们将有一个包含 2 个 bin 的直方图。第一个 bin 包含强度小于 125 的像素计数，第二个 bin 包含强度大于或等于 125 的像素计数。</p>
<p>在 SIFT 的上下文中，使用了方向直方图。这意味着 bin 表示角度而不是如上所述的强度。</p>
<p>因此具体bin表示什么要看直方图的描述。</p>
</div>
<div class="align-center figure align-default">
<img alt="../../../../../../_images/84.jpg" src="../../../../../../_images/84.jpg" />
</div>
<p>将第二幅图像中点 <span class="math notranslate nohighlight">\((x,y)\)</span> 在第一幅图像中对应的极线 <span class="math notranslate nohighlight">\(l\)</span> 的角度表示为 <span class="math notranslate nohighlight">\(\alpha(x,y) \in [0,\pi)\)</span></p>
<p>由于对极几何的性质，某些  <span class="math notranslate nohighlight">\(\alpha(x,y)\)</span> 的角度是不可能的。 因此，需要定义由有效角度组成的区间，将用多个区间覆盖作为  <span class="math notranslate nohighlight">\([a,b]\)</span> ，</p>
<div class="math notranslate nohighlight">
\[\begin{split}a = min(\alpha(0,0), \alpha(w_2,0), \alpha(0, h_2), \alpha(w_2, h_2))\\
b = max(\alpha(0,0), \alpha(w_2,0), \alpha(0, h_2), \alpha(w_2, h_2))\\\end{split}\]</div>
<p>点  <span class="math notranslate nohighlight">\((0, 0)\)</span> 是第二个图像的左上角， <span class="math notranslate nohighlight">\(w_2\)</span> 是它的宽度， <span class="math notranslate nohighlight">\(h_2\)</span> 是它的高度。</p>
<p>对点进行散列时，bin 的大小将为：</p>
<div class="math notranslate nohighlight">
\[\frac{b - a}{\# bins}\]</div>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>在实际操作中，这是很重要的一步，因为有时对极点远在图像之外，因此角度范围小于 1。</p>
<p>如果没有自适应 bin 大小计算，算法在这种情况下不会加速匹配。</p>
</div>
<div class="admonition attention">
<p class="admonition-title">注意</p>
<p>当对极落在图像内时， <span class="math notranslate nohighlight">\([a, b]\)</span>  为  <span class="math notranslate nohighlight">\([0,\pi)\)</span> 。 在进行传统的描述符匹配时，只考虑那些位于相应 bin 中且 Sampson 距离低于用于确定位姿的阈值的匹配。</p>
</div>
<p>执行引导后，对 2 到 30 个可能的候选对象而不是所有关键点进行描述符匹配。</p>
<p>应用具有自适应比率阈值的标准 SIFT 比率测试 ，这取决于最近邻居的数量—— pool越小，比率测试越严格。</p>
<p>如果找到一个好的位姿，则在 A* 之后应用匹配过程。 由于 A∗ 需要一组对应关系来确定位姿是否合理，
因此使用来自当前图像可见的 <strong>point tracks</strong> 的对应关系。
当成功匹配新图像对时，将计算并更新多视图轨迹。</p>
</div>
<div class="section" id="adaptive-correspondence-ranking">
<h2><span class="section-number">15.6. </span>Adaptive Correspondence Ranking<a class="headerlink" href="#adaptive-correspondence-ranking" title="永久链接至标题">¶</a></h2>
<p>在大规模问题中进行成对相对位姿估计时，自适应地设置 PROSAC 采样的点对应权重。
PROSAC 利用先验预测的点的内点概率等级，并从最有希望的点开始采样，然后再抽取不太可能导致所寻求模型的样本。</p>
<div class="admonition important">
<p class="admonition-title">重要</p>
<p>思想是基于这样一个事实：</p>
<blockquote>
<div><p>在匹配图像集合时，在一张图像中检测到并与其他图像匹配的特征点经常出现多次。</p>
</div></blockquote>
<p>因此，在 PROSAC 采样中首先使用包含早期inlier的对应点。 相反，先前图像中的outlier应该稍后绘制。</p>
</div>
<p>假设得到第 <span class="math notranslate nohighlight">\(t\)</span> 个图像对以匹配关键点 <span class="math notranslate nohighlight">\(K_i,K_j\)</span> 的集合。
来自任一组的每个关键点 <span class="math notranslate nohighlight">\(p\)</span> 的得分为 <span class="math notranslate nohighlight">\(s_p^{(t)}\)</span> ，用于确定其在所有关键点中的outlier等级。</p>
<p>在成功估计图像对的位姿 <span class="math notranslate nohighlight">\(P_{ij}\)</span> 后，得到 <span class="math notranslate nohighlight">\((p,q)\)</span> 的概率 <span class="math notranslate nohighlight">\(P((p,q)|P_{ij})\)</span> ，是给定位姿 <span class="math notranslate nohighlight">\(P_{ij}\)</span> 的异常值，
其中  <span class="math notranslate nohighlight">\((p, q)\)</span> 是一个暂定的对应关系， <span class="math notranslate nohighlight">\(p ∈ K_i，q ∈ K_j\)</span> 。</p>
<p>概率 <span class="math notranslate nohighlight">\(P((p,q)|P_{ij})\)</span> 可以计算，从假设正态或 χ2 分布的点到模型残差。 由于不知道概率 <span class="math notranslate nohighlight">\(P(p|P_{ij})\)</span> 和  <span class="math notranslate nohighlight">\(P(q|P_{ij})\)</span> 如何相关，
假设 p 和 q 与刚性重建一致是独立事件，因此 <span class="math notranslate nohighlight">\(P((p,q)|P_{ij}) = P(p|P_{ij})P(q|P_{ij})\)</span> 。</p>
<p>为了能够分解概率  <span class="math notranslate nohighlight">\(P ((p, q) | P_{ij})\)</span> ，假设  <span class="math notranslate nohighlight">\(P (p | P_{ij}) = P (q | P_{ij}) = P ((p, q) | P_{ij})\)</span> 。</p>
<p>然后，在第 t 个图像对匹配后，该概率用于更新分数 <span class="math notranslate nohighlight">\(s_p^{(t+1)} = s_p^{(t)} = P(p|P_{ij})\)</span> 和  <span class="math notranslate nohighlight">\(s_q^{(t+1)} = s_q^{(t)}P(q|P_{ij})\)</span></p>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>令 <span class="math notranslate nohighlight">\(s_p^{(0)} = 1\)</span> ，因为所有关键点在开始时都同样可能是异常值。</p>
</div>
</div>
<div class="section" id="experiments">
<h2><span class="section-number">15.7. </span>Experiments<a class="headerlink" href="#experiments" title="永久链接至标题">¶</a></h2>
<p>比较了位姿图生成算法，包括提出的基于 <span class="math notranslate nohighlight">\(A^*\)</span> 的技术。 比较的方法是：</p>
<ol class="arabic simple">
<li><p>标准的穷举匹配 (EM)，其中每个测试的图像对都由 FLANN + GC-RANSAC 匹配。</p></li>
</ol>
<ol class="arabic simple" start="2">
<li><p>一个最小生成树 (MST)，其中全局相似性分数用作权重。</p></li>
</ol>
<ol class="arabic simple" start="3">
<li><p>提议的基于 A* 的技术，如果可能，姿势来自由 A* 确定的路径。 否则，将应用标准匹配。</p></li>
</ol>
<ol class="arabic simple" start="4">
<li><p>广度优先 (BF) 遍历以与所提出的 A* 算法相同的方式应用。</p></li>
</ol>
<div class="align-center figure align-default">
<img alt="../../../../../../_images/92.jpg" src="../../../../../../_images/92.jpg" />
</div>
<div class="align-center figure align-default">
<img alt="../../../../../../_images/101.jpg" src="../../../../../../_images/101.jpg" />
</div>
<div class="align-center figure align-default">
<img alt="../../../../../../_images/1110.jpg" src="../../../../../../_images/1110.jpg" />
</div>
<div class="align-center figure align-default">
<img alt="../../../../../../_images/1210.jpg" src="../../../../../../_images/1210.jpg" />
</div>
<div class="align-center figure align-default">
<img alt="../../../../../../_images/138.jpg" src="../../../../../../_images/138.jpg" />
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="../Optimizing_the_Viewing_Graph/Optimizing_the_Viewing_Graph.html" class="btn btn-neutral float-right" title="16. Optimizing the Viewing Graph for Structure-from-Motion" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="../GlobalSfM_Application/GlobalSfM_Application.html" class="btn btn-neutral float-left" title="14. Global Structure-from-Motion and Its Application" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; 版权所有 2021, linzzz.

    </p>
  </div> 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>