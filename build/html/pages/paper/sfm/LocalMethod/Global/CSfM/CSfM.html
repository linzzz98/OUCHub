

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>9. CSFM: COMMUNITY-BASED STRUCTURE FROM MOTION &mdash; OUCHub  文档</title>
  

  
  <link rel="stylesheet" href="../../../../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../../_static/twemoji.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../../_static/custom.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../../../../" src="../../../../../../_static/documentation_options.js"></script>
        <script src="../../../../../../_static/jquery.js"></script>
        <script src="../../../../../../_static/underscore.js"></script>
        <script src="../../../../../../_static/doctools.js"></script>
        <script src="../../../../../../_static/language_data.js"></script>
        <script src="https://twemoji.maxcdn.com/v/latest/twemoji.min.js"></script>
        <script src="../../../../../../_static/twemoji.js"></script>
        <script src="../../../../../../_static/translations.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../../../../../_static/js/theme.js"></script>

    
    <link rel="index" title="索引" href="../../../../../../genindex.html" />
    <link rel="search" title="搜索" href="../../../../../../search.html" />
    <link rel="next" title="10. Very Large-Scale Global SfM by Distributed Motion Averaging" href="../Very_Large-Scale_Global_SfM/Very_Large-Scale_Global_SfM.html" />
    <link rel="prev" title="8. Line-based Robust SfM with Little Image Overlap" href="../../Incremental/Line_based_SfM/Line_based_SfM.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: black" >
          

          
            <a href="../../../../../../index.html" class="icon icon-home"> OUCHub
          

          
            
            <img src="../../../../../../_static/logo_1.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">基础知识</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../knowledge/k_MH.html">💊 Math</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../knowledge/k_CV.html">🍤 Computer Vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../knowledge/k_ML.html">🍎 Machine Learning</a></li>
</ul>
<p class="caption"><span class="caption-text">论文学习</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../../../../p_sfm.html">🍊 Structure from Motion</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../Incremental/Structure-from-Motion_Revisited/Structure-from-Motion_Revisited.html">1. Structure-from-Motion Revisited</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Incremental/Hierarchical_Structure-And-Motion/Hierarchical_Structure-And-Motion.html">2. Hierarchical structure-and-motion recovery from uncalibrated images</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Incremental/View-graph_SfM/View-graph_SfM.html">3. View-graph Selection Framework for SfM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Incremental/View-graph_Framework/View-graph_Framework.html">4. View-graph construction framework for robust and efficient structure-from-motion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Incremental/MarkerSfM/MarkerSfM.html">5. Improved Structure from Motion Using Fiducial Marker Matching</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Incremental/Privacy_SfM/Privacy_SfM.html">6. Privacy Preserving Structure-from-Motion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Incremental/SfM_with_Line_Segments/SfM_with_Line_Segments.html">7. Structure from Motion with Line Segments Under Relaxed Endpoint Constraints</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Incremental/Line_based_SfM/Line_based_SfM.html">8. Line-based Robust SfM with Little Image Overlap</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">9. CSFM: COMMUNITY-BASED STRUCTURE FROM MOTION</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#community-detection">9.1. COMMUNITY DETECTION</a></li>
<li class="toctree-l3"><a class="reference internal" href="#global-similarity-averaging">9.2. GLOBAL SIMILARITY AVERAGING</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#global-scale-averaging">9.2.1. Global Scale Averaging</a></li>
<li class="toctree-l4"><a class="reference internal" href="#global-rotation-averaging">9.2.2. Global Rotation Averaging</a></li>
<li class="toctree-l4"><a class="reference internal" href="#global-translation-averaging">9.2.3. Global Translation Averaging</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#reconstructions-merging">9.3. Reconstructions Merging</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../Very_Large-Scale_Global_SfM/Very_Large-Scale_Global_SfM.html">10. Very Large-Scale Global SfM by Distributed Motion Averaging</a></li>
<li class="toctree-l2"><a class="reference internal" href="../GlobalSfM_SA/GlobalSfM_SA.html">11. Global Structure-from-Motion by Similarity Averaging</a></li>
<li class="toctree-l2"><a class="reference internal" href="../GlobalSfM_Application/GlobalSfM_Application.html">12. Global Structure-from-Motion and Its Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Efficient_Initial_Pose-graph/Efficient_Initial_Pose-graph.html">13. Effcient Initial Pose-graph Generation for Global SfM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Optimizing_the_Viewing_Graph/Optimizing_the_Viewing_Graph.html">14. Optimizing the Viewing Graph for Structure-from-Motion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Reducing_Drift_Using_Extended_Feature/Reducing_Drift_Using_Extended_Feature.html">15. Reducing Drift in Structure From Motion Using Extended Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Reducing_Drift_Using_Extended_Feature/Reducing_Drift_Using_Extended_Feature.html#id1">16. 问题</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Hybrid/HSfM/HSfM.html">17. HSfM: Hybrid Structure-from-Motion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Hybrid/View_Graph_Construction/View_Graph_Construction.html">18. View-graph construction framework for robust and efficient structure-from-motion</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/A_Global_Linear_Method_for_Camera_Pose_Registration/A_Global_Linear_Method_for_Camera_Pose_Registration.html">19. A Global Linear Method for Camera Pose Registration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/Robust_Rotation_and_Translation_Estimation/Robust_Rotation_and_Translation_Estimation.html">20. Robust Rotation and Translation Estimation in Multiview Reconstruction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/FivePoint_Relative_Pose_Problem/FivePoint_Relative_Pose_Problem.html">21. An Effcient Solution to the Five-Point Relative Pose Problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/Hybrid_Camera_Estimation/Hybrid_Camera_Estimation.html">22. Hybrid Camera Pose Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/Lie_Averaging/Lie_Averaging.html">23. Lie-Algebraic Averaging For Globally Consistent Motion Estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/Efficient_Robust_Rotation_Averaging/Efficient_Robust_Rotation_Averaging.html">24. Efficient and Robust Large-Scale Rotation Averaging</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/Robust_Relative_Rotation_Averaging/Robust_Relative_Rotation_Averaging.html">25. Robust Relative Rotation Averaging</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/Privacy_Image_Queries/Privacy_Image_Queries.html">26. Privacy Preserving Image Queries for Camera Localization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/Privacy_Location/Privacy_Location.html">27. Privacy Preserving Image-Based Localization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/Using _Many_Cameras_as_One/Using _Many_Cameras_as_One.html">28. Using Many Cameras as One</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../CameraPose/Pose_Estimation_From_Points_Lines/Pose_Estimation_From_Points_Lines.html">29. Accurate and Linear Time Pose Estimation from Points and Lines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../DeepLearning/SfMLearner/SfMLearner.html">30. Unsupervised Learning of Depth and Ego-Motion from Video</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../DeepLearning/DeepSfM/DeepSfM.html">31. DeepSFM: Structure From Motion Via Deep Bundle Adjustment</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../p_slam.html">🍊 SLAM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../p_pointcloud.html">🍋 Point Cloud</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../p_others.html">🍒 Others</a></li>
</ul>
<p class="caption"><span class="caption-text">源码解析</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../code/colmap.html">🍑 Colmap</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../code/theiasfm.html">🍑 TheiaSfM</a></li>
</ul>
<p class="caption"><span class="caption-text">杂七杂八</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../others/o_others.html">🍺 Others</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../index.html">OUCHub</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../../../../p_sfm.html">🍊 Structure from Motion</a> &raquo;</li>
        
      <li><span class="section-number">9. </span>CSFM: COMMUNITY-BASED STRUCTURE FROM MOTION</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../../../../../_sources/pages/paper/sfm/LocalMethod/Global/CSfM/CSfM.rst.txt" rel="nofollow"> 查看页面源码</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="csfm-community-based-structure-from-motion">
<h1><span class="section-number">9. </span>CSFM: COMMUNITY-BASED STRUCTURE FROM MOTION<a class="headerlink" href="#csfm-community-based-structure-from-motion" title="永久链接至标题">¶</a></h1>
<p>在本文提出了一种基于 <strong>Community</strong> （以下翻译为社区）的自适应 SfM（CSfM）方法，该方法同时考虑了鲁棒性和效率，解决了三个凸 L1 优化问题。</p>
<ol class="arabic simple">
<li><p>对极几何图形被划分为单独的社区 。</p></li>
<li><p>并行解决每个社区的重建问题。</p></li>
<li><p>通过一种新颖的全局相似性平均方法合并重建结果。</p></li>
</ol>
<p>全局 SfM 方法同时从对极几何图 (EG) 计算相机位姿，其中顶点对应于图像和边缘链接匹配的图像对，并且仅执行BA调整 一次。
然而，那些基于基本矩阵的方法对极线几何异常值更敏感，它们只能校准平行刚性图中的图像。 造成的结果是，因为 EG 可能不够密集和准确，许多有用的图像可能会被丢弃。</p>
<p>传统的 SfM 方法通常将所有图像视为一个单独的社区，但对于大规模场景重建，特别是对于那些从互联网上搜索到的无序图像，图像分布通常具有社区特征：
一些地方得到图像更密集，而其他地方的图像更稀疏。在这种情况下，同时重建所有图像并不是一个合理的选择。</p>
<p>在本文中，提出了一种自动检测图像数据是否具有社区特征的方法，如果具有，则首先将图像划分为独立的社区，然后对每个社区并行进行重建，然后进行合并步骤将所有重建结果对齐到一个统一的全局框架中。</p>
<div class="admonition note">
<p class="admonition-title">注解</p>
<dl class="field-list simple">
<dt class="field-odd">贡献</dt>
<dd class="field-odd"><ol class="arabic simple">
<li><p>提出了一种自动社区检测方法来确定图像数据集是否应该划分为社区。</p></li>
<li><p>提出了一种社区图构建方法，将对极几何图划分为内部连接较密、外部连接较稀疏的组。</p></li>
<li><p>提出了一种新的全局相似性平均方法，将每个社区中所有单独的重建结果合并到一个统一的全局框架中。</p></li>
</ol>
</dd>
</dl>
</div>
<div class="align-center figure align-default">
<img alt="../../../../../../_images/129.jpg" src="../../../../../../_images/129.jpg" />
</div>
<div class="section" id="community-detection">
<h2><span class="section-number">9.1. </span>COMMUNITY DETECTION<a class="headerlink" href="#community-detection" title="永久链接至标题">¶</a></h2>
<p>社区检测已广泛用于复杂网络分析，目的是将图划分为内部连接较密集和外部连接较稀疏的子图。</p>
<div class="admonition attention">
<p class="admonition-title">注意</p>
<p>给定对极几何图（EG），其顶点对应图像，边缘链接匹配的图像对，目标是检测图像数据是否具有社区特征以及存在多少社区。</p>
</div>
<p>设 <span class="math notranslate nohighlight">\(A_{ij}\)</span> 是 EG 的相邻矩阵的一个元素。 如果相机 <span class="math notranslate nohighlight">\(i\)</span> 和相机 <span class="math notranslate nohighlight">\(j\)</span> 相连，则 <span class="math notranslate nohighlight">\(A_{ij} = 1\)</span> ，否则 <span class="math notranslate nohighlight">\(A_{ij} = 0\)</span> 。</p>
<p>令  <span class="math notranslate nohighlight">\(d_i = \sum\limits_j A_{ij}\)</span>  为 EG 中顶点 <span class="math notranslate nohighlight">\(i\)</span> 的度数，表示连接到第 <span class="math notranslate nohighlight">\(i\)</span> 个相机的相机数量， <span class="math notranslate nohighlight">\(m = \frac{1}{2} \sum\limits_{ij}A_{ij}\)</span> 是 EG 中的边数。</p>
<p>那么，如果对极边的存在是随机的，则连接相机 <span class="math notranslate nohighlight">\(i\)</span> 和相机 <span class="math notranslate nohighlight">\(j\)</span> 的边的存在概率为：</p>
<div class="math notranslate nohighlight">
\[\frac{d_i d_j}{2m}\]</div>
<p>为了测量随机图和 EG 之间的社区内连接分数的差异，使用《Finding com- munity structure in very large networks》中提出的模块化指标 Q。</p>
<p>假设摄像机 <span class="math notranslate nohighlight">\(i\)</span> 属于社区  <span class="math notranslate nohighlight">\(U_p\)</span> ，摄像机 <span class="math notranslate nohighlight">\(j\)</span> 属于社区 <span class="math notranslate nohighlight">\(U_q\)</span> ，则 <span class="math notranslate nohighlight">\(Q\)</span> 定义为：</p>
<div class="math notranslate nohighlight">
\[Q = \frac{1}{2m} \sum\limits_{ij}(A_{ij} - \frac{d_i d_j}{2m}) \delta (U_p, U_q)\]</div>
<p>如果  <span class="math notranslate nohighlight">\(U_p = U_q\)</span> ，则  <span class="math notranslate nohighlight">\(\delta (U_p, U_q) = 1\)</span> ，否则为 0。</p>
<p>为了划分 EG，假设每个节点首先属于一个单独的社区，然后当合并导致 Q 的最大增加时，单独的社区被迭代地加入。</p>
<p>在树状图的生成过程中，模块性有一个单一的峰值  <span class="math notranslate nohighlight">\(Q_{max}\)</span> ，这表明最重要的社区结构。在实践中发现  <span class="math notranslate nohighlight">\(Q_{max} &gt; 0.3\)</span> 表明 EG 具有显着的社区结构。
因此，当 Q 的峰值大于 0.3 时，取分区结果。 如果  <span class="math notranslate nohighlight">\(Q_{max} &lt; 0.3\)</span> ，则应将所有图像视为单个社区。</p>
<p>然而，对于大规模场景重建问题，一次性分区通常是不够的，因为一些当前分区仍然具有社区特征。 因此需要迭代地划分 EG，直到每个划分结果都可以被视为一个社区。
当迭代完成时，得到一个社区图，它的顶点对应社区，边连接社区。</p>
<div class="admonition attention">
<p class="admonition-title">注意</p>
<p>请注意，当一对社区之间存在一些极线边缘时，它们是相互关联的。</p>
</div>
<div class="align-center figure align-default">
<img alt="../../../../../../_images/220.jpg" src="../../../../../../_images/220.jpg" />
</div>
<p>考虑到场景重建的鲁棒性，每个社区都应该为后续的合并步骤重建足够的 3D 点，因此本文为每个社区设置了最小数量的图像 N</p>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>在本文工作中，N 设置为 20</p>
</div>
<p>迭代收敛后，将那些图像数量小于 N 的小数据集合并到它们最近的连接社区中。两个社区之间的紧密程度被定义为跨越它们的对极几何边的数量。</p>
</div>
<div class="section" id="global-similarity-averaging">
<h2><span class="section-number">9.2. </span>GLOBAL SIMILARITY AVERAGING<a class="headerlink" href="#global-similarity-averaging" title="永久链接至标题">¶</a></h2>
<p>社区检测后，每个社区的图像连接变得更加密集，因此可以使用许多 SfM 方法进行重建。社区通常构建一个连通图，因此本文提出了一种全局相似性平均方法来鲁棒地对齐这些单独的重建结果。</p>
<div class="section" id="global-scale-averaging">
<h3><span class="section-number">9.2.1. </span>Global Scale Averaging<a class="headerlink" href="#global-scale-averaging" title="永久链接至标题">¶</a></h3>
<p>设 <span class="math notranslate nohighlight">\(s_i\)</span> 为第 i 个社区的尺度， <span class="math notranslate nohighlight">\(s_{ij}\)</span> 为社区 <span class="math notranslate nohighlight">\(U_i\)</span> 和 <span class="math notranslate nohighlight">\(U_j\)</span> 之间 3D 相似性变换中的比例因子。</p>
<p>给定 <span class="math notranslate nohighlight">\(\frac{s_i}{s_j} = s_{ij}\)</span> ，通过取双方的对数函数：</p>
<div class="math notranslate nohighlight">
\[log(s_i) - log(s_j) = log(s_{ij})\]</div>
<p>通过从所有连接的社区对堆叠上述方程，得到一个线性方程组：</p>
<div class="math notranslate nohighlight">
\[A_s * x_s = b_s\]</div>
<p>其中  <span class="math notranslate nohighlight">\(x_s\)</span> 和 <span class="math notranslate nohighlight">\(b_s\)</span> 是分别连接  <span class="math notranslate nohighlight">\(log(s_i)\)</span>  和  <span class="math notranslate nohighlight">\(log(s_{ij})\)</span>  的向量， <span class="math notranslate nohighlight">\(A_s\)</span>  是一个稀疏矩阵，其中只有 1 和 -1。</p>
<p>由于尺度估计达到全局尺度，为了消除规范歧义，将第一个社区 <span class="math notranslate nohighlight">\(s_1\)</span> 的尺度设置为单位： <span class="math notranslate nohighlight">\(log(s_1) = 0\)</span> 。</p>
<p>然后，方程系统通过以下 L1 优化：</p>
<div class="math notranslate nohighlight">
\[||A_s ∗ x_s − b_s||_{L1}\]</div>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>请注意，此 L1 优化是凸的。</p>
</div>
</div>
<div class="section" id="global-rotation-averaging">
<h3><span class="section-number">9.2.2. </span>Global Rotation Averaging<a class="headerlink" href="#global-rotation-averaging" title="永久链接至标题">¶</a></h3>
<p>设 <span class="math notranslate nohighlight">\(R_i\)</span> 为第 <span class="math notranslate nohighlight">\(i\)</span> 个社区的旋转变换， <span class="math notranslate nohighlight">\(R_{ij}\)</span> 为社区 <span class="math notranslate nohighlight">\(U_i\)</span> 和 <span class="math notranslate nohighlight">\(U_j\)</span> 之间3D相似度变换中的相对旋转变换。</p>
<p>给定 <span class="math notranslate nohighlight">\(R_{ij} = R_j * R_i^T\)</span> 通过对两边取对数，得到它们对应的角度轴向量之间的等式：</p>
<div class="math notranslate nohighlight">
\[w_{ij} = w_j - w_i\]</div>
<p>其中 <span class="math notranslate nohighlight">\(w_{ij}\)</span> 是 <span class="math notranslate nohighlight">\(R_{ij}\)</span> 对应的角轴矢量， <span class="math notranslate nohighlight">\(w_i\)</span> 、 <span class="math notranslate nohighlight">\(w_j\)</span> 分别是 <span class="math notranslate nohighlight">\(R_i\)</span> 和 <span class="math notranslate nohighlight">\(R_j\)</span> 对应的角轴矢量。</p>
<p>通过从所有连接的社区对中叠加上述方程，得到了一个线性方程组，使用L1RA方法求解旋转平均。</p>
<p>然后对于第 <span class="math notranslate nohighlight">\(i\)</span> 个社区，给定其尺度  <span class="math notranslate nohighlight">\(s_i\)</span>  和旋转变换 <span class="math notranslate nohighlight">\(R_i\)</span> ，将其第 k 个场景点 <span class="math notranslate nohighlight">\(X_{ik}\)</span> 变换为  <span class="math notranslate nohighlight">\(s_i ∗ R_i ∗ X_{ik}\)</span> 。 在此转换之后，重新计算一对连接社区之间的位移  <span class="math notranslate nohighlight">\(T_{ij}\)</span> 。</p>
</div>
<div class="section" id="global-translation-averaging">
<h3><span class="section-number">9.2.3. </span>Global Translation Averaging<a class="headerlink" href="#global-translation-averaging" title="永久链接至标题">¶</a></h3>
<p>设 <span class="math notranslate nohighlight">\(T_i\)</span> 为第 <span class="math notranslate nohighlight">\(i\)</span> 个社区的平移变换， <span class="math notranslate nohighlight">\(T_{ij}\)</span> 为社区 <span class="math notranslate nohighlight">\(U_i\)</span> 和 <span class="math notranslate nohighlight">\(U_j\)</span> 之间 3D 相似变换中的相对平移变换。</p>
<p>类似地，通过从所有连接的社区对中叠加方程 <span class="math notranslate nohighlight">\(T_{ij} = T_j - T_i\)</span> ，就有了一个线性方程组，然后，该方程组通过以下 L1 优化求解：</p>
<div class="math notranslate nohighlight">
\[||A_t ∗ x_t − b_t||_{L1}\]</div>
<p>其中  <span class="math notranslate nohighlight">\(x_t\)</span> 和 <span class="math notranslate nohighlight">\(b_t\)</span> 是通过分别连接 <span class="math notranslate nohighlight">\(T_i\)</span> 和 <span class="math notranslate nohighlight">\(T_{ij}\)</span> 的向量，并且 <span class="math notranslate nohighlight">\(A_t\)</span> 是一个稀疏矩阵，其中非零值只有 1 和 -1。</p>
<p>由于位移估计取决于全局位移变换，为了消除规范歧义，将第一个社区的位移设置为零， <span class="math notranslate nohighlight">\(T_1 = 0\)</span> 。</p>
</div>
</div>
<div class="section" id="reconstructions-merging">
<h2><span class="section-number">9.3. </span>Reconstructions Merging<a class="headerlink" href="#reconstructions-merging" title="永久链接至标题">¶</a></h2>
<p>给定每个社区的尺度、旋转、平移变换，所有社区的重建结果（3D 场景点和相机位姿）被合并到一个单一的全局框架中。</p>
<p>对于第 <span class="math notranslate nohighlight">\(i\)</span> 个社区，其第 k 个 3D 场景点 <span class="math notranslate nohighlight">\(X_{ik}^o\)</span> 转换为：</p>
<div class="math notranslate nohighlight">
\[X_{ik}^g = X_{ik}^o + T_i\]</div>
<p>对于第 <span class="math notranslate nohighlight">\(i\)</span> 个社区中的第 <span class="math notranslate nohighlight">\(j\)</span> 个摄像机，其摄像机旋转 <span class="math notranslate nohighlight">\(R_{ij}^o\)</span> 和摄像机中心 <span class="math notranslate nohighlight">\(C_{ij}\)</span> 被转换为：</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{eqnarray}
   R_{ij}^g &amp;=&amp; R_{ij}^o * R_i^T\\
   C_{ij}^g &amp;=&amp; s_i * R_i * C_{ij}^o + T_i
\end{eqnarray}\end{split}\]</div>
<p>其中 <span class="math notranslate nohighlight">\(X_{ij}^g,R_{ij}^g,C_{ij}^g\)</span> 分别表示最终统一全局框架中的变换 3D 场景点、相机旋转和相机中心。 合并后，执行最终的BA调整以进一步细化所有相机位姿和场景点。</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="../Very_Large-Scale_Global_SfM/Very_Large-Scale_Global_SfM.html" class="btn btn-neutral float-right" title="10. Very Large-Scale Global SfM by Distributed Motion Averaging" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="../../Incremental/Line_based_SfM/Line_based_SfM.html" class="btn btn-neutral float-left" title="8. Line-based Robust SfM with Little Image Overlap" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; 版权所有 2021, linzzz.

    </p>
  </div> 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>